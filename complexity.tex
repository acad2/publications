\chapter{A glimpse at computational complexity (Yves Crama)}
\chplab{complexity}
In order to fully appreciate the field of combinatorial optimization, it is necessary to understand, at least at an intuitive level, some of the basic concepts of computational complexity. This part of theoretical computer science deals with fundamental, but extremely deep questions like: ``what tasks can be carried out by a computer?'', or ``how much time does a given computational task require?''. In this chapter, we attempt to introduce some elements of computational complexity, in a very informal and hand-waving way.

\section{Computational performance criteria}
What do we expect from an algorithm for a combinatorial optimization problem? Well, an obvious answer would be that this algorithm should always return an optimal solution of the problem. Is it the only game in town? Certainly not. We might also want it to be fast or efficient. Combining these two expectations is a crucial thing. Of course, the time required to solve a problem increases with the size of this problem,  where the size can be measured by the amount of data needed to describe a particular instance of theproblem.

\begin{example}
Let us take a look at an example. Suppose that we want to solve a 0-1 linear programming problem involving $n$ variables $x_j\in\accl{0,1}$, $j=1,\ldots,n$. We can certainly find an optimal solution by listing all possible vectors $\tupl{x_1,x_2,\ldots,x_n}$, checking for each of them whether it is feasible or not, computing the value of the objective function for each such feasible solution, and, finally, retaining the best solution found in the process. If we decide to go that way, then we must consider $2^n$ vectors. For $n=50$, that means $2^{50}\approx 10^{15}=1'000'000'000'000'000$ vectors! If our algorithm is able to enumerate one million ($1'000'000$) solutions per second, the whole procedure takes $10^9$ seconds, or about $30$ years. And for $n=60$, the enumeration of the $2^{60}$ solutions would take about $30'000$ years !!

\paragraph{}
Notice that adding $10$ variables to the problem increases the computing time by a multiplicative factor of $2^{10}\approx1'000$. So, with $n=80$ variables (a rather modest problem size), the same algorithm would run for $30$ billion years, which is about twice the age of the universe. Not really efficient, by any practical standards...
\end{example}

\paragraph{}
Let us look at this issue from another vantage point. Consider the well-known Moore's law: Gordon Moore, co-founder of the chips giant Intel, prophetized in 1965 that the number of transistors per square inch on integrated circuits would double every 18 months per year starting from 1962, the year the integrated circuit was invented (see the original paper for more details). In other words, your PC processor works twice faster every year and a half, meaning that its speed is multiplied by $100$ in $10$ years\footnote{This rate has slowed down since Moore made his claim. It is now generally admitted that the number of transistors doubles every two years.}. So, if you were able to enumerate $2^n$ solutions in one hour in 1993, you can enumerate $100\cdot2^n<2{n+7}$ solutions in 2003. This great increase in computing speed thus allows you to ``gain'' only $7$ variables in 10 years!! Conclusion: exhaustive enumeration is not feasible in practice for large-scale (or even medium-scale) combinatorial optimization problems. Furthermore, we should not count on technical progress alone to improve the situation in any significant way. Only algorithmic, or mathematical, advances can help in this respect.

\paragraph{}
So, how much progress can we expect on the theoretical front? Before we provide a tentative answer to this question, let us try to formulate more precisely some of the notions that have just been sketched.

\section{Problems and problem instances}
Formally speaking, a (computational) \concept{problem} is a generic question whose formulation includes a number of undetermined parameters. Here are some simple examples.

\begin{description}
 \item[Matrix addition problem] The parameters are $n$, $A$ and $B$ where $n\in\NNN$, and $A$ and $B$ are two $n\times n$ matrices. Question: what is the value of $A+B$?
 \item[Shortest path problem] The parameters are a graph $G=\tupl{V,E}$, two vertices $s,t\in V$, and the length $\fun{l}{e}\geq 0$ of every edge $e\in E$. Question: find a shortest path from $s$ to $t$.
 \item[Traveling salesman problem] The parameters are a graph $G=\tupl{V,E}$, and the length $\fun{l}{e}\geq 0$ of every edge $e\in E$. Question: find a shortest traveling salesman tour in $G$.
\end{description}

\paragraph{}
An \concept{instance} of a problem $P$ arises when the values of all undetermined parameters of $P$ are specified (or, more intuitively, by specifying the input file that contains the numerical data). So, a problem can also be viewed as a collection of instances. Notice that an instance admits an answer, but a problem does not (try to give the answer of the matrix addition problem above!). We will use the symbol $I$ for an instance.

\paragraph{}
An \concept{algorithm} for a problem $P$ is a step-by-step procedure that describes how to compute a solution for every instance $I$ of $P$. To compare the efficiency of different algorithms for a same problem $P$ , we can determine the time required by each algorithm to solve an instance of $P$. Notice that this obviously depends on the particular instance which is to be solved, but also on the speed of the computer, on the skills of the programmer, etc. Therefore, we need again to define this notion in more formal way.

\paragraph{}
The \concept{size of an instance} $I$, denoted by $\fun{s}{I}$, is the number of bits needed to represent $I$. It is determined both by the number of parameters and by their magnitude. (Intuitively, this can be viewed as the size of the input file of a computer program which solves $I$.)

\paragraph{}
The \concept{running time} of an algorithm $A$ on an instance $I$, denoted $\fun{t_A}{I}$, is the number of elementary operations (additions, multiplications, comparisons,...) performed by $A$ when it runs on the instance $I$. Determining the running time of an algorithm for each particular instance $I$ is not an easy task. However, it is often easier to estimate the running time as a function of the size of the instance.

\paragraph{}
Consider again the examples defined above.
\begin{description}
 \item[Matrix addition problem] Instance size: $\approx n^2$.
 \begin{enumerate}
  \item Naive addition: $\approx n^2$ (additions). We denote this by $\bigoh{n^2}$, meaning that the running time grows at most like $n^2$.
 \end{enumerate}
 \item[Shortest path problem] Instance size: $\bigoh{n^2}$ where $n=\abs{V}$.
 \begin{enumerate}
  \item Enumerate all possible paths between $s$ and $t$. There could be exponentially many paths, leading to $\bigoh{2^n}$.
  \item \concept{Dijkstra's algorithm}: $\bigoh{n^2}$ operations.
 \end{enumerate}
 \item[Traveling salesman problem] Instance size: $\bigoh{n^2}$ where $n=\abs{V}$.
 \begin{enumerate}
  \item Enumerate all possible tours: $\bigoh{n!}$.
 \end{enumerate}
\end{description}

\paragraph{}
Notice that, in all these examples, we chose to ignore the size of a number, that is, we did not take into account the number of bits needed to store some number. In view of these examples, we are led to the following concept: the complexity of an algorithm $A$ for a problem $P$ is the function 
\begin{equation}
\fun{f_A}{n}=\max\condset{\fun{t_A}{I}}{\mbox{$I$ is an instance of $P$ with size $\fun{s}{I}=n$}}.
\end{equation}
This is sometimes called the ``worst-case complexity'' of $A$: indeed, the definition focuses on the worst-case running time of $A$ on an instance of size $n$, rather than on its average running.

\section{Easy and hard problems}

\begin{figure}[hbt]
\centering
\importtikzsubfigure{cpx-lin}{Linear: $\fun{F}{n}=a\cdot n+b$.}
\importtikzsubfigure{cpx-exp}{exponential: $\fun{F}{n}=a\cdot 2^n$.}
\caption{Different types of complexity behavior.}
\figlab{cpx}
\end{figure}

\figref{cpx} represents different types of complexity behaviors for algorithms. The algorithm $A$ is polynomial if $\fun{F_A}{n}$ is a polynomial (or is bounded by a polynomial) in $n$ and exponential if $\fun{F_A}{n}$ grows faster than any polynomial process in $n$. Intuitively, we can probably accept the idea that a polynomial algorithm is more efficient than an exponential one.

\paragraph{}
For instance, the obvious algorithms for the addition and or the multiplication of matrices are polynomial. So is the Gaussian elimination algorithm for the solution of systems of linear equations. On the other hand, the simplex method (or at least, some variants of it) for linear programming problems is known to be exponential\footnote{Klee and Minty provide instances I of the LP problem such that $\fun{t_{\mbox{\tiny simplex}}}{I}\geq 2^{\fun{s}{I}}$.} while interior point methods are polynomial. This clearly illustrates the emphasis on the ``worst-case'' running time which was already underlined above: indeed, in an average sense, the simplex algorithm is an efficient method.

\paragraph{}
The complete enumeration approach for shortest path, matching, stable set or traveling salesman problems is exponential, since all these problems have an exponential number of feasible solutions. But polynomial algorithms exist for the shortest path problem and the matching problem.

\paragraph{}
For stable set, the traveling salesman problem, or for 0-1 integer programming problems, by contrast, only exponential algorithms are known. In fact, it is widely suspected that there does not exist any polynomial algorithm for these problems. This is a typical feature of so-called \concept{NP-hard} problems which we define (very informally again) as follows.

\begin{definition}[NP-hard]
A problem $P$ is NP-hard if it is as least as difficult as the 0-1 linear programming problem, in the sense that any algorithm for $P$ can be used to solve the 0-1 LP problem with a polynomial increase in running time.
\end{definition}

\paragraph{}
The next claim has resisted all proof attempts (and there have been many) since the early 70's, but the vast majority of computer scientists and operations researchers believe that it holds true.

\begin{conjecture}[$P\neq NP$]
If a problem is N P -hard, then it cannot be solved by a polynomial algorithm.
\end{conjecture}

\paragraph{}
The $P\neq NP$ conjecture, if true, expresses a deep and fundamental fact of complexity theory. Its implications are of enormous importance for the development of algorithms in operations research and related areas. Indeed, a large number of combinatorial optimization problems turn out to be NP-hard, and hence difficult to solve.

\begin{proposition}
\prplab{nphard-problems}
The following problems are NP-hard:
\begin{enumerate}
 \item \concept{traveling salesman};
 \item \concept{stable set};
 \item \concept{graph coloring};
 \item \concept{knapsack};
 \item \concept{assembly line balancing};
 \item \concept{three-dimensional assignment};
 \item \concept{facility location};
 \item \concept{jobshop scheduling};
 \item several hundred other combinatorial optimization problems.
\end{enumerate}
\end{proposition}

It is quite remarkable that most of the problems in the above list can in fact be formulated as special cases of the 0-1 LP problem. This is obvious, in particular, for the knapsack problem, but it is also true (though less obvious) for graph equipartitioning, or for the traveling salesman problem, or for graph coloring, etc. Thus, the actual meaning of \prpref{nphard-problems} is that all these NP-hard problems are somehow equivalent, in the sense that an efficient algorithm for any of them would immediately provide an efficient algorithm for all of them.

\paragraph{}
From a practical point of view, however, some N P -hard problems turn out to be more difficult than others. For instance, the knapsack problem is quite easy to solve as compared with the general 0-1 LP problems. Nevertheless, NP-hard problems seem to be intrinsically tougher than linear systems, LP problems or shortest path problems. As a consequence, for the solution of such difficult (or ``apparently difficult'') problems, heuristic algorithms are often used in practice. In addition, of course, not all combinatorial optimization problems are NP-hard. \concept{Minimal spanning tree}, \concept{min-cost flow}, \concept{linear optimization} are prime examples where polynomial-time algorithms suffice to find an optimal solution.

\paragraph{}
\begin{definition}[Heuristic]
A \concept{heuristic} for an optimization problem $P$ is an algorithm which is based on intuitively appealing principles, but which does not guarantee to provide an optimal solution of $P$.
\end{definition}

So, when running on a particular combinatorial optimization problem, a heuristic could for instance:
\begin{enumerate}
 \item return an optimal solution of the problem; or
 \item return a suboptimal solution; or
 \item return an infeasible solution; or
 \item fail to return any solution at all;
 \item etc.
\end{enumerate}
This very broad definition of a heuristic may seem rather amazing at first sight. It raises again the question of the criteria which can be applied to analyze the performance of a particular heuristic. We mention here two criteria which will be of particular concern in this course.

\subsection{Computational complexity}
\ssclab{ccpx}
Generally speaking, we want heuristics to be fast, at least when compared with the highly exponential running times mentioned above. In fact, the main reason for giving up optimality is that we want the heuristic to compute quickly a reasonably good solution. Thus, the basic trade-off that we want to achieve reads
\begin{quote}
Solution quality versus running time
\end{quote}

\subsection{Quality of approximation}
\ssclab{qoa}
The solution returned by the heuristic should provide a good approximation of the optimal solution. To understand how to measure this, let $x^H$ be the solution computed by heuristic $H$ for a particular instance and let $x^{\star}$ be an optimal solution for this instance. Also, $\funm{val}{x^H}$ denotes the value of the solution found by the heuristic, and $\funm{val}{x^{\star}}$ denotes the optimum solution value. Then
\begin{equation}
\fun{E}{x^H}=\displaystyle\frac{\funm{val}{x^H}-\funm{val}{x^{\star}}}{\funm{val}{x^{\star}}}\geq 0
\end{equation}
provides a relative error measure: the closer it is to $0$, the better the solution $x^H$. Notice that we assume that we are dealing with a minimization problem.

\paragraph{}
In general, however, $\funm{val}{x^{\star}}$ is unknown. So, suppose now that we know how to compute a \concept{lower bound} on $\funm{val}{x^{\star}}$, i.e. a number $v^-$ such that $v^-\leq\funm{val}{x^{\star}}$ (this is often much easier to compute than $\funm{val}{x^{\star}}$). Define
\begin{equation}
\fun{E^-}{x^H}=\displaystyle\frac{\funm{val}{x^H}-v^-}{v^-}
\end{equation}
Then we have
\begin{equation}
\fun{E}{x^H}=\displaystyle\frac{\funm{val}{x^H}}{\funm{val}{x^{\star}}}-1\leq\displaystyle\frac{\funm{val}{x^H}}{v^-}-1=\fun{E^-}{x^H}
\end{equation}
which means that $\fun{E^-}{x^H}$ overestimates the relative error $\fun{E}{x^H}$. So, if $\fun{E^-}{x^H}$ is small, we can certainly be happy with the quality of the solution provided by $H$.
\begin{note}
Notice also that if the lower bound $v^-$ is reasonably close to $\funm{val}{x^{\star}}$, then $\fun{E^-}{x^H}$ actually provides a good estimate of the error.
\end{note}

\paragraph{}
\begin{example}
For example, consider the traveling salesman instance described by the (symmetric) distance matrix $L$, where $l_{i,j}$ represents the distance from $i$ to $j$, with $i,j\in\accl{1,2,\ldots,6}$:
\begin{equation}
L=\mtrx{cccccc}{
0&4&7&2&6&3\\
4&0&3&5&5&7\\
7&3&0&2&6&5\\
2&5&2&0&9&8\\
6&5&6&9&0&5\\
3&7&5&8&5&0
}
\end{equation}


Assume now that a heuristic returns the tour $x^H=\tupl{1,2,3,4,5,6}$ (displayed in \figref{heuristic-tsp-ex}).

\importtikzfigure{heuristic-tsp-ex}{A feasible tour.}

The total length of this tour is $\funm{val}{x^H}=4+3+2+9+5+3=26$. On the other hand, an obvious lower bound on the optimal tour length is given by the sum of the $6$ shortest distances in $6$. Thus $v^âˆ’=2+2+3+3+4+5=19$, and, consequently, $\fun{E^-}{x^H}=\tfrac{26-19}{19}\approx 0.37$. We can therefore conclude that $x^H$ is at most $37\%$ longer than the optimal tour.
\end{example}

\paragraph{}
In order to compute lower bounds for combinatorial optimization problems, a simple but powerful principle can often be used: when a constraint of a minimization problem $P$ is relaxed (i.e., when the constraint is either removed or replaced by a weaker one), then the optimal value of the resulting ``relaxed'' problem provides a lower bound on the optimal value of $P$. This principle will be illustrated on the examples below.
\section{Exercises}
\begin{exercise}
\exclab{ch3ex1}
Consider again the traveling salesman problem. For every vertex $v\in V$, select the shortest edge $e_v$ incident to $v$. Show that $\isumdomain[v]{V}{\fun{l}{e_v}}$ is a lower bound on the length of the optimal tour.
\paragraph{}
Compute this lower bound for the numerical example in \sscref{ccpx}. Can you improve this lower bound by taking into account the two shortest edges incident to every vertex $v$? What bound do you obtain for the numerical example?
\end{exercise}
\begin{exercise}
\exclab{ch3ex2}
Consider the following problem: you want to save $n$ electronic files with respective sizes $s_1,s_2,\ldots,s_n\geq 0$ on the smallest possible number of storing devices (say, floppy disks) with capacity $C$.
This problem is known under the name of \concept{bin packing problem}, and it is NP-hard. Can you compute a lower bound on its optimal value?
\end{exercise}
\begin{exercise}
\exclab{ch3ex3}
Show that the optimal value of the \concept{linear programming} problem
\begin{eqnarray}
\mbox{minimize}&\vec{c}\cdot\vec{x}\\
\mbox{subject to}&A\cdot\vec{x}\leq\vec{b}\\
&\forall j\in1,2,\ldots,n:0\leq x_j\leq 1
\end{eqnarray}
provides a lower bound on the optimal value of the \concept{0-1 linear programming} problem
\begin{eqnarray}
\mbox{minimize}&\vec{c}\cdot\vec{x}\\
\mbox{subject to}&A\cdot\vec{x}\leq\vec{b}\\
&\forall j\in1,2,\ldots,n:x_j\in\accl{0,1}
\end{eqnarray}
\end{exercise}
\begin{exercise}
Show that the lower bounds obtained in \excrefr{ch3ex1}{ch3ex2} can all be viewed as optimal solutions of a relaxation of the original problem.
\end{exercise}
\begin{exercise}
Consider the \concept{Hamiltonian cycle} problem (HC). Given a graph $G=\tupl{V,E}$, does $G$ contain a Hamiltonian cycle, i.e., a cycle visiting each node exactly once? Show that the \concept{Traveling salesman} Problem is at least as hard as the Hamiltonian cycle problem.
\end{exercise}
\begin{exercise}
Consider the \concept{Partition} problem. Given are $n$ integers $s_1,s_2,\ldots,s_n$, does there exist a set $S\subseteq{1,2,\ldots,n}$ such that $\isumdomain[i]{S}{s_i}=\isumndomain[i]{S}{s_i}$? Show that the \concept{Knapsack} problem is at least as hard as Partition.
\end{exercise}
\begin{exercise}
Show that \concept{min-cost flow} is at least as hard as \concept{shortest path}.
\end{exercise}