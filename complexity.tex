\chapter{A Glimpse at Computational Complexity (Yves Crama)}
\chplab{complexity}
In order to fully appreciate the field of \concept{combinatorial optimization}, it is necessary to understand, at least at an intuitive level, some of the basic concepts of \concept{computational complexity}. This part of \concept{theoretical computer science} deals with fundamental, but extremely deep questions like: ``what tasks can be carried out by a computer?'', or ``how much time does a given computational task require?''. In this chapter, we attempt to introduce some elements of \concept{computational complexity}, in a very informal and hand-waving way.

\section{Computational Performance Criteria}
What do we expect from an \concept{algorithm} for a \concept{combinatorial optimization problem}? Well, an obvious answer would be that this \concept{algorithm} should always return an \concept{optimal solution} of the problem. Is it the only game in town? Certainly not. We might also want it to be fast or \concept{efficient}. Combining these two expectations is a crucial thing. Of course, the time required to solve a problem increases with the size of this problem,  where the size can be measured by the amount of data needed to describe a particular instance of the problem.

\begin{example}
Let us take a look at an example. Suppose that we want to solve a 0-1 \concept{linear programming problem} involving $n$ variables $x_j\in\accl{0,1}$, $j=1,\ldots,n$. We can certainly find an \concept{optimal solution} by listing all possible \concepts{vector} $\tupl{x_1,x_2,\ldots,x_n}$, checking for each of them whether it is \concept{feasible} or not, computing the value of the \concept{objective function} for each such \concept{feasible solution}, and, finally, retaining the best solution found in the process. If we decide to go that way, then we must consider $2^n$ \concepts{vector}. For $n=50$, that means $2^{50}\approx 10^{15}=1'000'000'000'000'000$ \concepts{vector}! If our \concept{algorithm} is able to enumerate one million ($1'000'000$) solutions per second, the whole procedure takes $10^9$ seconds, or about $30$ years. And for $n=60$, the enumeration of the $2^{60}$ solutions would take about $30'000$ years !!

\paragraph{}
Notice that adding $10$ \concepts{variable} to the problem increases the \concept{computing time} by a multiplicative factor of $2^{10}\approx1'000$. So, with $n=80$ variables (a rather modest problem size), the same \concept{algorithm} would run for $30$ billion years, which is about twice the age of the universe. Not really efficient, by any practical standards...
\end{example}

\paragraph{}
Let us look at this issue from another vantage point. Consider the well-known \concept{Moore's law}: Gordon Moore, co-founder of the chips giant Intel, prophetized in 1965 that the number of transistors per square inch on integrated circuits would double every 18 months per year starting from 1962, the year the integrated circuit was invented (see the original paper for more details). In other words, your PC processor works twice faster every year and a half, meaning that its speed is multiplied by $100$ in $10$ years\footnote{This rate has slowed down since Moore made his claim. It is now generally admitted that the number of transistors doubles every two years.}. So, if you were able to enumerate $2^n$ solutions in one hour in 1993, you can enumerate $100\cdot2^n<2{n+7}$ solutions in 2003. This great increase in computing speed thus allows you to ``gain'' only $7$ variables in 10 years!! Conclusion: exhaustive enumeration is not feasible in practice for large-scale (or even medium-scale) combinatorial optimization problems. Furthermore, we should not count on technical progress alone to improve the situation in any significant way. Only algorithmic, or mathematical, advances can help in this respect.

\paragraph{}
So, how much progress can we expect on the theoretical front? Before we provide a tentative answer to this question, let us try to formulate more precisely some of the notions that have just been sketched.

\section{Problems and Problem Instances}
Formally speaking, a (computational) \concept{problem} is a generic question whose formulation includes a number of undetermined \concepts{parameter}. Here are some simple examples.

\begin{description}
 \item[\concept{Matrix addition problem}] The \concept{parameter} are $n$, $A$ and $B$ where $n\in\NNN$, and $A$ and $B$ are two $n\times n$ \concept[matrix]{matrices}. Question: what is the value of $A+B$?
 \item[\concept{Shortest path problem}] The \concept{parameter} are a graph $G=\tupl{V,E}$, two vertices $s,t\in V$, and the length $\fun{l}{e}\geq 0$ of every edge $e\in E$. Question: find a \concept{shortest path} from $s$ to $t$.
 \item[\concept{Traveling salesman problem}] The \concept{parameter} are a graph $G=\tupl{V,E}$, and the length $\fun{l}{e}\geq 0$ of every edge $e\in E$. Question: find a \concept{shortest traveling salesman tour} in $G$.
\end{description}

\paragraph{}
An \concept{instance} of a \concept{problem} $P$ arises when the values of all undetermined \concepts{parameter} of $P$ are specified (or, more intuitively, by specifying the input file that contains the numerical data). So, a \concept{problem} can also be viewed as a collection of \concepts{instance}. Notice that an \concept{instance} admits an answer, but a problem does not (try to give the answer of the \concept{matrix addition problem} above!). We will use the symbol $I$ for an \concept{instance}.

\paragraph{}
An \concept{algorithm} for a \concept{problem} $P$ is a step-by-step procedure that describes how to compute a \concept{solution} for every \concept{instance} $I$ of $P$. To compare the efficiency of different algorithms for a same problem $P$, we can determine the time required by each algorithm to solve an instance of $P$. Notice that this obviously depends on the particular instance which is to be solved, but also on the speed of the computer, on the skills of the programmer, etc. Therefore, we need again to define this notion in more formal way.

\paragraph{}
The \concept{size of an instance} $I$, denoted by $\fun{s}{I}$, is the number of bits needed to represent $I$. It is determined both by the number of parameters and by their magnitude. (Intuitively, this can be viewed as the size of the input file of a computer program which solves $I$.)

\paragraph{}
The \concept{running time} of an algorithm $A$ on an instance $I$, denoted $\fun{t_A}{I}$, is the number of \concepts{elementary operation} (additions, multiplications, comparisons,...) performed by $A$ when it runs on the instance $I$. Determining the \concept{running time} of an algorithm for each particular instance $I$ is not an easy \concept{task}. However, it is often easier to estimate the running time as a function of the size of the instance.

\paragraph{}
Consider again the examples defined above.
\begin{description}
 \item[Matrix addition problem] Instance size: $\approx n^2$.
 \begin{enumerate}
  \item Naive addition: $\approx n^2$ (additions). We denote this by $\bigoh{n^2}$, meaning that the running time grows at most like $n^2$.
 \end{enumerate}
 \item[Shortest path problem] Instance size: $\bigoh{n^2}$ where $n=\abs{V}$.
 \begin{enumerate}
  \item Enumerate all possible paths between $s$ and $t$. There could be exponentially many paths, leading to $\bigoh{2^n}$.
  \item \concept{Dijkstra's algorithm}: $\bigoh{n^2}$ operations.
 \end{enumerate}
 \item[Traveling salesman problem] Instance size: $\bigoh{n^2}$ where $n=\abs{V}$.
 \begin{enumerate}
  \item Enumerate all possible tours: $\bigoh{n!}$.
 \end{enumerate}
\end{description}

\paragraph{}
Notice that, in all these examples, we chose to ignore the size of a number, that is, we did not take into account the number of bits needed to store some number. In view of these examples, we are led to the following concept: the \concept{complexity} of an algorithm $A$ for a problem $P$ is the function 
\begin{equation}
\fun{f_A}{n}=\max\condset{\fun{t_A}{I}}{\mbox{$I$ is an instance of $P$ with size $\fun{s}{I}=n$}}.
\end{equation}
This is sometimes called the ``\concept{worst-case complexity}'' of $A$: indeed, the definition focuses on the \concept{worst-case running time} of $A$ on an instance of size $n$, rather than on its \concept{average running time}.

\section{Easy and Hard Problems}

\begin{figure}[hbt]
\centering
\importtikzsubfigure{cpx-lin}{Linear: $\fun{F}{n}=a\cdot n+b$.}
\importtikzsubfigure{cpx-exp}{exponential: $\fun{F}{n}=a\cdot 2^n$.}
\caption{Different types of complexity behavior.}
\figlab{cpx}
\end{figure}

\figref{cpx} represents different types of \concept{complexity} behaviors for \concepts{algorithm}. The \concept{algorithm} $A$ is \concept{polynomial} if $\fun{F_A}{n}$ is a \concept{polynomial} (or is bounded by a \concept{polynomial}) in $n$ and \concept{exponential} if $\fun{F_A}{n}$ grows faster than any \concept{polynomial} process in $n$. Intuitively, we can probably accept the idea that a \concept{polynomial} \concept{algorithm} is more efficient than an \concept{exponential} one.

\paragraph{}
For instance, the obvious \concepts{algorithm} for the \concept[matrix addition problem]{addition} and or the \concept[matrix multiplication problem]{multiplication} of \concept[matrix]{matrices} are \concept{polynomial}. So is the \concept{Gaussian elimination algorithm} for the solution of systems of \concepts{linear equation}. On the other hand, the \concept{simplex method} (or at least, some variants of it) for \concepts{linear programming problem} is known to be \concept{exponential}\footnote{Klee and Minty provide instances I of the LP problem such that $\fun{t_{\mbox{\tiny simplex}}}{I}\geq 2^{\fun{s}{I}}$.} while \concepts{interior point method} are \concept{polynomial}. This clearly illustrates the emphasis on the ``\concept{worst-case running time}'' which was already underlined above: indeed, in an average sense, the \concept{simplex algorithm} is an efficient method.

\paragraph{}
The complete enumeration approach for \concept[shortest path problem]{shortest path}, \concept[matching problem]{matching}, \concept[stable set problem]{stable set} or \concept{traveling salesman problems} is \concept{exponential}, since all these \concepts{problem} have an exponential number of \concepts{feasible solution}. But \concepts{polynomial algorithm} exist for the \concept{shortest path problem} and the \concept{matching problem}.

\paragraph{}
For the \concept{stable set problem}, the \concept{traveling salesman problem}, or for \concepts{0-1 integer programming problem}, by contrast, only \concepts{exponential algorithm} are known. In fact, it is widely suspected that there does not exist any \concept{polynomial algorithm} for these problems. This is a typical feature of so-called \concept{NP-hard} problems which we define (very informally again) as follows.

\begin{definition}[NP-hard]
A problem $P$ is \concept{NP-hard} if it is as least as difficult as the \concept{0-1 linear programming problem}, in the sense that any \concept{algorithm} for $P$ can be used to solve the \concept{0-1 LP problem} with a \concept{polynomial} increase in \concept{running time}.
\end{definition}

\paragraph{}
The next claim has resisted all proof attempts (and there have been many) since the early 70's, but the vast majority of computer scientists and operations researchers believe that it holds true.

\begin{conjecture}[$P\neq NP$]
If a problem is \concept{NP-hard}, then it cannot be solved by a \concept{polynomial algorithm}.
\end{conjecture}

\paragraph{}
The \concept{$P\neq NP$ conjecture}, if true, expresses a deep and fundamental fact of \concept{complexity theory}. Its implications are of enormous importance for the development of algorithms in operations research and related areas. Indeed, a large number of \concepts{combinatorial optimization problem} turn out to be \concept{NP-hard}, and hence difficult to solve.

\begin{proposition}
\prplab{nphard-problems}
The following problems are \concept{NP-hard}:
\begin{enumerate}
 \item \concept[traveling salesman problem]{traveling salesman};
 \item \concept[stable set problem]{stable set};
 \item \concept[graph coloring problem]{graph coloring};
 \item \concept[knapsack problem]{knapsack};
 \item \concept[assembly line problem]{assembly line balancing};
 \item \concept[three-dimensional assignment problem]{three-dimensional assignment};
 \item \concept[facility location problem]{facility location};
 \item \concept[job shop scheduling problem]{job shop scheduling};
 \item several hundred other \concepts{combinatorial optimization problem}.
\end{enumerate}
\end{proposition}

It is quite remarkable that most of the problems in the above list can in fact be formulated as special cases of the \concept{0-1 LP problem}. This is obvious, in particular, for the \concept{knapsack problem}, but it is also true (though less obvious) for \concept[graph equipartitioning problem]{graph equipartitioning}, or for the \concept{traveling salesman problem}, or for \concept[graph coloring problem]{graph coloring}, etc. Thus, the actual meaning of \prpref{nphard-problems} is that all these \concept{NP-hard} problems are somehow equivalent, in the sense that an efficient algorithm for any of them would immediately provide an \concept{efficient algorithm} for all of them.

\paragraph{}
From a practical point of view, however, some \concept{NP-hard} problems turn out to be more difficult than others. For instance, the \concept{knapsack problem} is quite easy to solve as compared with the general \concepts{0-1 LP problem}. Nevertheless, \concept{NP-hard} problems seem to be intrinsically tougher than \concepts{linear system}, \concepts{linear programming problem} or \concepts{shortest path problem}. As a consequence, for the solution of such difficult (or ``apparently difficult'') problems, \concepts{heuristic algorithm} are often used in practice. In addition, of course, not all \concepts{combinatorial optimization problem} are \concept{NP-hard}. \concept{Minimal spanning tree}, \concept{min-cost flow}, \concept{linear optimization} are prime examples where \concept{polynomial time} algorithms suffice to find an \concept{optimal solution}.

\paragraph{}
\begin{definition}[Heuristic]
A \concept{heuristic} for an \concept{optimization problem} $P$ is an \concept{algorithm} which is based on intuitively appealing principles, but which does not guarantee to provide an \concept{optimal solution} of $P$.
\end{definition}

So, when running on a particular \concept{combinatorial optimization problem}, a \concept{heuristic} could for instance:
\begin{enumerate}
 \item return an \concept{optimal solution} of the problem; or
 \item return a \concept{suboptimal solution}; or
 \item return an \concept{infeasible solution}; or
 \item fail to return any \concept{solution} at all;
 \item etc.
\end{enumerate}
This very broad definition of a \concept{heuristic} may seem rather amazing at first sight. It raises again the question of the criteria which can be applied to analyze the performance of a particular \concept{heuristic}. We mention here two criteria which will be of particular concern in this course.

\subsection{Computational Complexity}
\ssclab{ccpx}
Generally speaking, we want \concepts{heuristic} to be fast, at least when compared with the highly exponential running times mentioned above. In fact, the main reason for giving up optimality is that we want the \concept{heuristic} to compute quickly a \concept{reasonably good solution}. Thus, the basic trade-off that we want to achieve reads
\begin{quote}
Solution quality versus running time
\end{quote}

\subsection{Quality of Approximation}
\ssclab{qoa}
The \concept{solution} returned by the \concept{heuristic} should provide a good \concept{approximation} of the \concept{optimal solution}. To understand how to measure this, let $x^H$ be the \concept{solution} computed by \concept{heuristic} $H$ for a particular instance and let $x^{\star}$ be an \concept{optimal solution} for this instance. Also, $\funm{val}{x^H}$ denotes the value of the \concept{solution} found by the \concept{heuristic}, and $\funm{val}{x^{\star}}$ denotes the \concept{optimum solution} value. Then
\begin{equation}
\fun{E}{x^H}=\displaystyle\frac{\funm{val}{x^H}-\funm{val}{x^{\star}}}{\funm{val}{x^{\star}}}\geq 0
\end{equation}
provides a \concept{relative error measure}: the closer it is to $0$, the better the solution $x^H$. Notice that we assume that we are dealing with a \concept{minimization problem}.

\paragraph{}
In general, however, $\funm{val}{x^{\star}}$ is unknown. So, suppose now that we know how to compute a \concept{lower bound} on $\funm{val}{x^{\star}}$, i.e. a number $v^-$ such that $v^-\leq\funm{val}{x^{\star}}$ (this is often much easier to compute than $\funm{val}{x^{\star}}$). Define
\begin{equation}
\fun{E^-}{x^H}=\displaystyle\frac{\funm{val}{x^H}-v^-}{v^-}
\end{equation}
Then we have
\begin{equation}
\fun{E}{x^H}=\displaystyle\frac{\funm{val}{x^H}}{\funm{val}{x^{\star}}}-1\leq\displaystyle\frac{\funm{val}{x^H}}{v^-}-1=\fun{E^-}{x^H}
\end{equation}
which means that $\fun{E^-}{x^H}$ overestimates the \concept{relative error} $\fun{E}{x^H}$. So, if $\fun{E^-}{x^H}$ is small, we can certainly be happy with the \concept[quality of a solution]{quality of the solution} provided by $H$.
\begin{note}
Notice also that if the \concept{lower bound} $v^-$ is reasonably close to $\funm{val}{x^{\star}}$, then $\fun{E^-}{x^H}$ actually provides a good estimate of the \concept{error}.
\end{note}

\paragraph{}
\begin{example}
For example, consider the \concept[traveling salesman problem]{traveling salesman} instance described by the (\concept[symmetric distance matrix]{symmetric}) \concept{distance matrix} $L$, where $l_{i,j}$ represents the \concept{distance} from $i$ to $j$, with $i,j\in\accl{1,2,\ldots,6}$:
\begin{equation}
L=\mtrx{cccccc}{
0&4&7&2&6&3\\
4&0&3&5&5&7\\
7&3&0&2&6&5\\
2&5&2&0&9&8\\
6&5&6&9&0&5\\
3&7&5&8&5&0
}
\end{equation}


Assume now that a \concept{heuristic} returns the \concept{tour} $x^H=\tupl{1,2,3,4,5,6}$ (displayed in \figref{heuristic-tsp-ex}).

\importtikzfigure{heuristic-tsp-ex}{A feasible tour.}

The total \concept[length of a tour]{length of this tour} is $\funm{val}{x^H}=4+3+2+9+5+3=26$. On the other hand, an obvious \concept{lower bound} on the \concept{optimal tour length} is given by the sum of the $6$ shortest \concepts{distance} in $6$. Thus $v^âˆ’=2+2+3+3+4+5=19$, and, consequently, $\fun{E^-}{x^H}=\tfrac{26-19}{19}\approx 0.37$. We can therefore conclude that $x^H$ is at most $37\%$ longer than the \concept{optimal tour}.
\end{example}

\paragraph{}
In order to compute \concepts{lower bound} for \concepts{combinatorial optimization problem}, a simple but powerful principle can often be used: when a \concept{constraint} of a \concept{minimization problem} $P$ is relaxed (i.e., when the \concept{constraint} is either removed or replaced by a weaker one), then the optimal value of the resulting ``\concept{relaxed problem}'' provides a \concept{lower bound} on the \concept{optimal value} of $P$. This principle will be illustrated on the examples below.
\section*{Exercises}
\begin{exercise}
\exclab{ch3ex1}
Consider again the \concept{traveling salesman problem}. For every \concept{vertex} $v\in V$, select the \concept{shortest edge} $e_v$ \concept{incident} to $v$. Show that $\isumdomain[v]{V}{\fun{l}{e_v}}$ is a \concept{lower bound} on the length of the \concept{optimal tour}.
\paragraph{}
Compute this \concept{lower bound} for the numerical example in \sscref{ccpx}. Can you improve this lower bound by taking into account the two \concepts{shortest edge} \concept{incident} to every \concept{vertex} $v$? What bound do you obtain for the numerical example?
\end{exercise}
\begin{exercise}
\exclab{ch3ex2}
Consider the following problem: you want to save $n$ \concepts{electronic file} with respective sizes $s_1,s_2,\ldots,s_n\geq 0$ on the smallest possible number of \concepts{storing device} (say, floppy disks) with capacity $C$. This problem is known under the name of \concept{bin packing problem}, and it is \concept{NP-hard}. Can you compute a \concept{lower bound} on its \concept{optimal value}?
\end{exercise}
\begin{exercise}
\exclab{ch3ex3}
Show that the optimal value of the \concept{linear programming problem}
\begin{eqnarray}
\mbox{minimize}&\vec{c}\cdot\vec{x}\\
\mbox{subject to}&A\cdot\vec{x}\leq\vec{b}\\
&\forall j\in1,2,\ldots,n:0\leq x_j\leq 1
\end{eqnarray}
provides a \concept{lower bound} on the \concept{optimal value} of the \concept{0-1 linear programming problem}
\begin{eqnarray}
\mbox{minimize}&\vec{c}\cdot\vec{x}\\
\mbox{subject to}&A\cdot\vec{x}\leq\vec{b}\\
&\forall j\in1,2,\ldots,n:x_j\in\accl{0,1}
\end{eqnarray}
\end{exercise}
\begin{exercise}
Show that the \concepts{lower bound} obtained in \excrefr{ch3ex1}{ch3ex2} can all be viewed as \concepts{optimal solution} of a \concept{relaxation} of the original problem.
\end{exercise}
\begin{exercise}
Consider the \concept{Hamiltonian cycle problem} (HC). Given a \concept{graph} $G=\tupl{V,E}$, does $G$ contain a \concept{Hamiltonian cycle}, i.e., a \concept{cycle} visiting each \concept{node} exactly once? Show that the \concept{traveling salesman Problem} is at least as hard as the \concept{Hamiltonian cycle problem}.
\end{exercise}
\begin{exercise}
Consider the \concept{Partition} problem. Given are $n$ integers $s_1,s_2,\ldots,s_n$, does there exist a set $S\subseteq{1,2,\ldots,n}$ such that $\isumdomain[i]{S}{s_i}=\isumndomain[i]{S}{s_i}$? Show that the \concept{knapsack problem} is at least as hard as the \concept{partition problem}.
\end{exercise}
\begin{exercise}
Show that \concept{min-cost flow problem} is at least as hard as \concept{shortest path problem}.
\end{exercise}