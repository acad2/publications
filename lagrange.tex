\chapter{Lagrangian Relaxation}
\chplab{lagrange}
In this chapter we study a concept called \concept{Lagrangian relaxation}. The formulation of many practical \concepts{combinatorial optimization problem} contains several sets of \concepts{constraint}. \concept{Lagrangian relaxation} exploits this property by disregarding one or more sets of \concepts{constraint}. It turns out that this \concept{relaxation} allows one to obtain \concepts{lower bound} (\concepts{upper bound}) for difficult minimization (maximization) problems. In \secref{lagrangeterminology} we introduce some terminology, \secref{lagrangeresult} presents some basic results, in \secref{lagrangeapplication} we describe an application, and \secref{lagrangedual} concludes this chapter by presenting two ways of strengthening the \concept{Lagrangian dual}.

\section{Terminology}
\seclab{lagrangeterminology}

Consider an \concept{integer program}:

\begin{equation}
z_{ip}=\funcs{\max}{\vec{c}\cdot\vec{x}}{x\in S},\mbox{ where }S=\condset{\vec{x}\in\ZZZ^n}{\vec{x}\geq\vec{0}\wedge A\cdot\vec{x}\leq\vec{b}},
\end{equation}

which can be rewritten as an \concept{integer problem} (IP):

\begin{eqnarray}
\mbox{maximize}&z_{ip}=\vec{c}\cdot\vec{x}\eqnlab{lagragea-m}\\
\mbox{subject to}&A_1\cdot\vec{x}\leq\vec{b}_1\eqnlab{lagragea-c1}\\
&A_2\cdot\vec{x}\leq\vec{b}_2\eqnlab{lagragea-c2}\\
&\vec{x}\geq\vec{0}\eqnlab{lagragea-c3}\\
&\vec{x}\in\ZZZ^n\eqnlab{lagragea-c4}
\end{eqnarray}

We are going to assume that $A_2\cdot\vec{x}\leq\vec{b}_2$ are $m-m_1$ ``\concepts{nice constraint}'', say those of an \concept[assignment problem]{assignment} or a \concept{network problem}. By simply dropping the $m_1$ ``\concepts{complicating constraint}'' $A_1\cdot\vec{x}\leq\vec{b}_1$, we obtain a \concept{relaxation} of the \concept{integer problem} (\concept{IP}) that is obviously easier to solve than the problem itself. There are many problems for which the constraints can be \concept[constraint partitioning]{partitioned} in this way. An example will be given in \secref{lagrangedual}.

\paragraph{}
The idea of dropping constraints can be embedded in a more general framework called \concept{Lagrangian relaxation}. It is convenient to consider a generalization of problem (IP) called \concept{IP(Q)}, which we formulate as follows:

\begin{eqnarray}
\mbox{maximize}&z_{ip}=\vec{c}\cdot\vec{x}\eqnlab{lagrageb-m}\\
\mbox{subject to}&A_1\cdot\vec{x}\leq\vec{b}_1\eqnlab{lagrageb-c1}\\
&\vec{x}\in Q\eqnlab{lagrageb-c2}
\end{eqnarray}

However, when we are discussing results that are specific to \concept{IP}, it is assumed that $Q=\condset{x\inZZZ^n}{\vec{x}\geq\vec{0}\wedge A_2\cdot\vec{x}=\vec{b}_2}\neq\emptyset$. Of course, the problem obtained from \concept{IP(Q)} by dropping the \concepts{complicating constraint}, $A_1\cdot\vec{x}\leq\vec{b}_1$ is much easier to solve than \concept{IP(Q)}. Now, for any $\lambda\in\brak{\RRR^+}^{m_1}$, consider the problem \concept{LR($\vec{\lambda}$)}:

\begin{equation}
\fun{z_{lr}}{\vec{\lambda}}=\funcs{\max}{\fun{z}{\vec{\lambda},\vec{x}}}{x\in Q},\mbox{ where }\fun{z}{\vec{\lambda},\vec{x}}=\vec{c}\cdot\vec{x}+\vec{\lambda}\cdot\brak{\vec{b}_1-A_1\cdot\vec{x}}.
\end{equation}

The problem \concept{LR($\vec{\lambda}$)} is called the \concept{Lagrangian relaxation} of \concept{IP(Q)} with respect to $A_1\cdot\vec{x}\leq\vec{b}_1$. This terminology is used because the vector $\lambda$ plays a role in \concept{LR($\vec{\lambda}$)} similar to the role of \concepts{Lagrange multiplier} in \concepts{constrained optimization problem}. By our choice, \concept{LR($\vec{\lambda}$)} does not contain the \concepts{complicating constraint}. Instead we have included these constraints in the \concept{objective function} with the ``\concept{penalty term}'' $\vec{\lambda}\cdot\brak{\vec{b}_1-A_1\cdot\vec{x}}$. Since $\vec{\lambda}\geq\vec{0}$, violations of $A_1\cdot\vec{x}\leq\vec{b_1}$ make the \concept{penalty term} negative, and thus, intuitively speaking, for suitably large values of $\vec{\lambda}$, one would expect that $A_1\cdot\vec{x}\leq\vec{b}_1$ will be satisfied.

\paragraph{}
Let us formally state the relation between $z_{ip}$ and $\fun{z_{lr}}{\vec{\lambda}}$:
\begin{theorem}
$\fun{z_{lr}}{\vec{\lambda}}\geq z_{ip}$ for all $\vec{\lambda}\geq\vec{0}$.
\begin{proof}
If $\vec{x}$ is feasible in \concept{IP(Q)}, then $\vec{x}\in Q$ and hence $\vec{x}$ is feasible for \concept{LR($\vec{\lambda}$)}. Also, $\fun{z}{\vec{\lambda},\vec{x}}=\vec{c}\cdot\vec{x}+\vec{\lambda}\cdot\brak{\vec{b}_1-A_1\cdot\vec{x}}\geq\vec{c}\cdot\vec{x}$ for all $\vec{x}$ feasible in \concept{IP(Q)} since $A_1\cdot\vec{x}\leq\vec{b}_1$ and $\vec{\lambda}\geq\vec{0}$.
\end{proof}
\end{theorem}

\paragraph{}
Obviously, one is interested in the \concept{least upper bound} from the infinite family of \concept[lagrange relaxation]{relaxations} $\accl{\funm{LR}{\vec{\lambda}}}_{\vec{\lambda}\geq\vec{0}}$,
denoted here by $\fun{z_{lr}}{\vec{\lambda}^{\star}}$, where $\vec{\lambda}^{\star}$ is an \concept{optimal solution} to the problem called \concept{LD}:

\begin{equation}
z_{ld}=\fun{\min_{\vec{\lambda}\geq\vec{0}}}{\fun{z_{lr}}{\vec{\lambda}}}.
\end{equation}

Problem \concept{LD} is called the \concept{Lagrangian dual} of \concept{IP(Q)} with respect to the \concepts{constraint} $A_1\cdot\vec{x}\leq\vec{b_1}$.

\section{Some results}
\seclab{lagrangeresult}
In this section we illustrate the terminology introduced in the previous section with the following example and use this example to derive some results.

\begin{example}
Consider the following problem.

\begin{eqnarray}
\mbox{maximize}&7\cdot x_1+2\cdot x_2		\eqnlab{lagragec-m}\\
\mbox{subject to}&-x_1+2\cdot x_2\leq4		\eqnlab{lagragec-c1}\\
&6\cdot x_1+x_2\leq24				\eqnlab{lagragec-c2}\\
&-2\cdot x_1-2\cdot x_2\leq-7			\eqnlab{lagragec-c3}\\
&-x_1\leq-2					\eqnlab{lagragec-c4}\\
&x_2\leq4					\eqnlab{lagragec-c5}\\
&x_1,x_2\in\ZZZ^+				\eqnlab{lagragec-c6}
\end{eqnarray}

Let $Q=\condset{\vec{x}\in\brak{\ZZZ^+}^2}{\vec{x}\mbox{ satisfies \eqnnrefe{lagragec-c1,lagragec-c2,lagragec-c3,lagragec-c4,lagragec-c5,lagragec-c6}}}$. The \concept{Lagrangian relaxation} (see \secref{lagrangeterminology}) with respect to $-x_1+2\cdot x_2\leq 4$ is: $\fun{z_{lt}}{\lambda}=\funf{\max_{x\in Q}}{7\cdot x_1+2\cdot x_2+\lambda\cdot\brak{4+x_1-2\cdot x_2}}$ or equivalently:

\begin{eqnarray}
\mbox{maximize}&\brak{7+\lambda}\cdot x_1+\brak{2-2\cdot\lambda}\cdot x_2+4\cdot\lambda\eqnlab{lagraged-m}\\
\mbox{subject to}&6\cdot x_1+x_2\leq24		\eqnlab{lagraged-c1}\\
&-2\cdot x_1-2\cdot x_2\leq-7			\eqnlab{lagraged-c2}\\
&-x_1\leq-2					\eqnlab{lagraged-c3}\\
&x_2\leq4					\eqnlab{lagraged-c4}\\
&x_1,x_2\in\ZZZ^+				\eqnlab{lagraged-c5}
\end{eqnarray}

Notice that $Q$ is a finite set of points, which can be written as follows (see \figref{lagrange-region-ex}):
\begin{equation}
\tupl{\vec{q}_1,\vec{q}_2,\vec{q}_3,\vec{q}_4,\vec{q}_5,\vec{q}_6,\vec{q}_7,\vec{q}_8}=\tupl{\tupl{2,2},\tupl{2,3},\tupl{2,4},\tupl{3,1},\tupl{3,2},\tupl{3,3},\tupl{3,4},\tupl{4,0}}.
\end{equation}
\end{example}

\importtikzfigure{lagrange-region-ex}{The $Q$ region of the Lagrange relaxation example.}

The example suggests at least two different \concepts{viewpoint}. The first one is to see $\fun{z}{\vec{\lambda},\vec{x}}=\brak{\vec{c}-\lambda\cdot A_1}\cdot\vec{x}+\vec{\lambda}\cdot\vec{b}_1$ as a \concept{linear function} of $\vec{x}$ for fixed $\vec{\lambda}$. It then follows that $\fun{z_{lr}}{\vec{\lambda}}$ can be determined by solving the \concept{linear program}.

\begin{equation}
\fun{z_{lr}}{\vec{\lambda}}=\funcs{\max}{\fun{z}{\vec{\lambda},\vec{x}}}{\vec{x}\in\funm{conv}{Q}}.
\end{equation}

In this example,

\begin{equation}
\funm{conv}{Q}=\condset{\vec{x}\in\brak{\RRR^+}^2}{-x_1\leq-2\wedge x_2\leq4\wedge-x_1-x_2\leq-4\wedge4\cdot x_1+x_2\leq16}.
\end{equation}

In \figref{lagrange-region-ex}, solid lines indicate the original constraints, the dots correspond to the \concept[feasible integral vertex]{feasible integral vertices}, and the dashed lines correspond to constraints describing $\funm{conv}{Q}$.

\paragraph{}
Thus, computing $\fun{z_{lr}}{\lambda}$ for $\lambda=0$ and $\lambda=1$ gives:

\begin{eqnarray}
\fun{z_{lr}}{0}=\funcs{\max}{7\cdot x_1+2\cdot x_2}{\vec{x}\in\funm{conv}{Q}}=\fun{z}{0,\vec{q}_7}=\fun{z}{0,\tupl{3,4}}=29\\
\fun{z_{lr}}{0}=\funcs{\max}{8\cdot x_1+4}{\vec{x}\in\funm{conv}{Q}}=\fun{z}{1,\vec{q}_8}=\fun{z}{1,\tupl{4,0}}=36
\end{eqnarray}

As one increases $\lambda$ from $0$, $\fun{z_{lr}}{\lambda}$ first decreases until $\lambda=\dfrac{1}{9}$ and then it increases. In general we obtain


\begin{equation}
\fun{z_{lr}}{\lambda}=\acclguard{\fun{z}{\lambda,\vec{q}_7}=29-\lambda&\xif0\leq\lambda\leq\dfrac{1}{9}\\\fun{z}{\lambda,\vec{q}_8}=28+8\cdot\lambda&\xif\dfrac{1}{9}<\lambda\leq1}.
\end{equation}

Hence, we can deduce that $z_{ld}=\fun{z_{lr}}{\dfrac{1}{9}}=\fun{z}{\dfrac{1}{9},\vec{q}_7}=\fun{z}{\dfrac{1}{9},\vec{q}_8}=\dfrac{260}{9}$ and $\lambda^{\star}=\dfrac{1}{9}$. Notice that, for $\lambda=\dfrac{1}{9}$, $\vec{q}_7$ as well as $\vec{q}_8$ are optimal with respect to the constraints determining $\funm{conv}{Q}$, and hence the \concept{objective function} - which equals $\dfrac{64}{9}\cdot x_1+\dfrac{16}{9}\cdot x_2$ - must be parallel to $4\cdot x_1+x_2\leq16$. All these calculations can be seen in \figref{lagrange-region-ex}.

\paragraph{}
The second \concept{viewpoint} is to consider $\fun{z_{lr}}{\vec{\lambda}}$ to be determined by \concept{maximization} over a set of \concepts{discrete point}, that is,

\begin{equation}
\fun{z_{lr}}{\vec{\lambda}}=\fun{\max_{\vec{q}_i\in Q}}{\fun{z}{\vec{\lambda},\vec{q}_i}}.
\end{equation}

Observe here that for a fixed $\vec{q}_i$ , $\fun{z}{\vec{\lambda},\vec{q}_i}=\vec{c}\cdot\vec{q}_i+\vec{\lambda}\cdot\brak{\vec{b}_1-A_1\cdot\vec{q}_i}$ is a \concept{linear function} of $\vec{\lambda}$. See \figref{lagrange-dual-ex}, where we have drawn the \concepts{linear function} $\fun{z}{\lambda,q_i}$ for $q_i\in Q$ for the leading example.


\importtikzfigure{lagrange-dual-ex}{The lines: (a) 18 + 2λ; (b) 20; (c) 22 - 2λ; (d) 23 + 5λ; (e) 25 + 3λ; (f) 27 + λ; (g) 29 - λ; (h) 28 + 8λ.}

In Figure 7.2 one can read the values of zLR (λ) for any value of λ. We see that zLR (λ) is piecewise linear
and convex (the heavy lines in Figure 7.2) and that zLD = 28 98 . Formally, one solves the linear program

zLR (λ) = min {w| w ≥ z(λ, xi ) for i = 1, . . . , 8},

which shows that zLR (λ) is the maximum of a finite number of linear functions and is therefore piecewise
linear and convex.
We now study how the solution of the Lagrangian dual relates to the solution of the original problem
IP(Q). Returning to Figure 7.1, notice that when λ = 1/9 we obtain

28

8
9

1
1
= z( , x7 ) = z( , x8 )
9
9
1 8 7 1 8
= z( , x + x )
9 9
9
1
1 8
= z( , (3, 4) + (4, 0))
9 9
9
1 28 32
1
28 32
= z( , ( , ) = z( , x∗ ) with x∗ = ( , )
9 9 9
9
9 9
1
∗
∗
∗
= cx + (4 + x1 − 2x2 )
9
∗
= cx .

In other words, by taking a convex combination of points in Q (in this example x7 and x8 ), we obtain a
point x∗ in conv(Q) satisfying the complicating constraint, for which cx∗ = zLD . This shows that for the
example we get zLD = max {cx| A1 x ≤ b1 , x ∈ conv(Q)}. And in fact this holds in general as witnessed
by the following theorem which we state without proof.

Theorem 7.2
zLD = max {cx| A1 x ≤ b1 , x ∈ conv(Q)}.

An interesting question is of course: how good is the bound zLD ? In general, the difference between
zLD and zIP (called the duality gap) depends on the sizes of conv(S) (which determines zIP ), conv(Q) ∩
91

{x| A1 x ≤ b1 } (which determines zLD ) and the objective coefficients c. A duality gap of 0 can be
characterized as follows.

Theorem 7.3 zLD = zIP for all c if and only if
n
n
conv{Q ∩ {x ∈ R+
| A1 x ≤ b1 }} = conv(Q) ∩ {x ∈ R+
| A1 x ≤ b1 }.

Another interesting difference is the difference between zLD and the value of the LP-relaxation, denoted
n
by zLP . Notice that this only makes sense when Q = {x ∈ Z+
| A2 x ≤ b2 }. We can characterize the case

where zLP = zLD .

n
Theorem 7.4 zLD = zLP for all c if all the extreme points of {x ∈ R+
| A2 x ≤ b2 } are integral.

It is easily verified that the conditions mentioned in the two previous theorems are not fulfilled by our
2
example. Indeed, we have 28 = zIP < zLD = 28 98 < zLP = 30 11
. (Check this !).

In fact, a more natural choice of complicating constraints in our example would lead to different results
2
2
for zLD . If we set Q = {x ∈ Z+
| − x1 ≤ −2, x2 ≤ 4}, we find that {x ∈ R+
| − x1 ≤ −2, x2 ≤ 4} only has

integral extreme points so that by our latest theorem, this Lagrangian relaxation would terminate with
2
.
zLD = zLP = 30 11

Summarizing, we have

n
n
conv(S) ⊆ conv(Q) ∩ {x ∈ R+
| A1 x ≤ b1 } ⊆ {x ∈ R+
| Ax ≤ b}.

This implies that zIP ≤ zLD ≤ zLP . But because some faces of the respective polyhedra can coincide,
we may obtain zIP = zLD or zLD = zLP for a particular c even if the conditions of the two previous
theorems do not hold. Below, we give at table indicating the possibilities using four different objective
functions c1 , c2 , c3 and c4 .
92

Objective functions

objective values

c1

zIP = zLD = zLP

2

zIP < zLD = zLP

c3

zIP < zLD < zLP

4

zIP = zLD < zLP

c

c

\section{An application}
\seclab{lagrangeapplication}

Suppose there is a set of n jobs to be assigned to a set of n workers, with N = {1, . . . , n}. Suppose
further that
• cij is the value of assigning worker i to job j,
• tij is the cost of training worker i to do job j, and
• there is a training budget of b units.
We wish to maximize the total value of the assignment subject to the budget constraint, that is

max

cij

xij

(7.8)

xij

=

1 for i ∈ N

(7.9)

xij

=

1 for j ∈ N

(7.10)

tij xij

≤

b

(7.11)

x

∈

{0, 1}.

(7.12)

i∈N j∈N

j∈N

i∈N

j∈N i∈N

If we wish to use Lagrangian relaxation there are different options to consider. Notice that in each of
the following four options the relaxed problem LR(λ) is considerably easier to solve than the original
problem.
1
1. Lagrangian relaxation with respect to (7.11). Then LR1 (λ), λ ∈ R+
is an assignment problem with

objective function
93

(cij − λtij )xij .

λb +
i∈N j∈N

2. Lagrangian relaxation with respect to (7.9) and (7.10). Then LR2 (u, v), u ∈ Rn , v ∈ Rn is a
knapsack problem with objective function

ui +
i∈N

(cij − ui − vj )xij .

vj +
j∈N

i∈N j∈N

3. Lagrangian relaxation with respect to (7.9) or (7.10), say (7.9). Then LR3 (u), u ∈ Rn is a knapsack
problem with so-called generalized upper bound constraints and with objective function

(cij − ui )xij .

ui +
i∈N

i∈N j∈N

4. Lagrangian relaxation with respect to (7.9) or (7.10) and (7.11), say (7.9) and (7.11). Only gener1
with objective function
alized upper bound constraints remain. Thus, LR4 (u, λ), u ∈ Rn , λ ∈ R+

λb +

(cij − ui − λtij )xij ,

ui +
i∈N

i∈N j∈N

which is trivial to solve. (For each j, an i is chosen to maximize cij −ui −λtij , and the corresponding
xij is set to 1).

In choosing a relaxation there are two major questions to consider: how strong is the lower bound zLD
and how difficult to solve is the Lagrangian dual (LD)? Let us here only consider the bounds.
When Q is a set of assignment constraints or a set of generalized upper bound constraints, we have that
4
1
= zLP . Since
= zLD
zLD

94

2

Q3 = {x ∈ {0, 1}n |

xij = 1 for j ∈ N,
i∈N

tij xij ≤ b}
i∈N j∈N

2

⊂ Q2 = {x ∈ {0, 1}n |

tij xij ≤ b}
i∈N j∈N

2

n
and conv(Q2 ) ⊂ {x ∈ R+
|

tij xij ≤ b, xij ≤ 1 for i, j ∈ N },
i∈N j∈N

we have
3
2
1
4
zIP ≤ zLD
≤ zLD
≤ zLD
= zLD
= zLP ,

and each of the inequalities is strict for some objective function.

\section{Strengthening the Lagrangian dual}
\seclab{lagrangedual}

We now consider two ways of strengthening the Lagrangian dual of problem IP. The first approach yields
a dual whose optimal value equals
n
n
max{cx| x ∈ conv(x ∈ Z+
| A1 x ≤ b1 ) ∩ conv(x ∈ Z+
| A2 x ≤ b2 )}.

This dual is obtained by applying Lagrangian duality to a reformulation of IP, which is called RIP:
zIP = max cx1
A1 x1

≤ b1

A2 x2

≤ b2

x1 − x2

= 0

x1

∈

n
Z+

x2

∈

n
Z+
.

95

Taking now x1 − x2 = 0 as complicating constraints, we obtain the Lagrangian dual of RIP:
zCSD = minu {max{(c

−

u)x1 + ux2 }}

A1 x1

≤

b1

A2 x2

≤

b2

x1

∈

n
Z+

x2

∈

n
Z+

= minc1 +c2 =c {

max

c1 x1 + max c2 x2 }

A1 x1

≤

b1

A2 x2

≤

b2

x1

∈

n
Z+

x2

∈

n
Z+
,

where u = c2 .
A polyhedral interpretation of the dual is stated in the next theorem.

n
n
| A2 x ≤ b2 
| A1 x ≤ b1  ∩ convx ∈ Z+
Theorem 7.5 zCSD = maxcx| x ∈ convx ∈ Z+

and zCSD ≤ zLD .

The technique described is referred to here with CS since this technique has been called cost splitting.
The technique is useful when

n
n
| A1 x ≤ b1 , so for some objective functions c we obtain
| A1 x ≤ b1  ⊂ x ∈ R+
• convx ∈ Z+

zCSD < zLD .
• The sets of constraints Ai x ≤ bi are simple to deal with separately; that is, the difficulty is caused
by their interaction.

In our example, we could take A1 x ≤ b1 to be constraint set (7.9) and (7.11) and take A2 x ≤ b2 to be
3
with the inequality strict for some objective
constraint sets (7.10) and (7.11). This yields zCSD ≤ zLD

functions c.
96

Another approach that domiantes the Lagrangian dual is the “surrogate” dual. Starting from IP(Q),
m1
with weights λ ∈ R+
for the complicating constraints, consider the following problem called SD(λ).

zSD (λ) = max{cx| λA1 x ≤ λb1 , x ∈ Q}.

The problem SD(λ) is called the surrogate relaxation of IP(Q) with respect to A1 x ≤ b1 . SD(λ) contains
n
a single complicating constraint. For instance when Q = Z+
the surrogate relaxation is a knapsack

problem. The surrogate dual of IP(Q) is the problem denoted by SD.

zSD = min

λ≥0 zSD (λ).

Although the surrogate dual can be used computationally, it does not have such nice theoretical properties
as the Lagrangian dual.

\section*{Exercises}
\begin{exercise}
Consider the following problem.
max 2x1 + 5x2
4x1 + x2

≤

28

x1 + 4x2

≤

27

x1 − 5x2

≤

1

x

∈

2
Z+
.

(i) Show that if any two constraints are dualized, the value of the Lagrangian dual equals the value of
the LP-relaxation.
(ii) Find an objective function for which (i) is false.
97

(iii) Show that if any single constraint is dualized, the value of the Lagrangian dual is an improvement
compared to the value of the LP-relaxation.
(iv) Apply cost-splitting to get a better Lagrangian dual.
\end{exercise}

\begin{exercise}
Construct two Lagrangian duals for the generalized assignment problem and discuss their merits.

max
s.t.

j cij xij

i
j

xij ≤ 1 for i ∈ M

i li xij

≤ bj for j ∈ N
2

x ∈ {0, 1}n .
\end{exercise}