\chapter{Approximation algorithms}
\chplab{approx}

\section{Introduction}
Instances of \concepts{combinatorial optimization problem} cannot always be solved to optimality within a reasonable amount of computing time. Indeed, some combinatorial problems are so-called \concept{NP-hard} (e.g. \concept[knapsack problem]{knapsack}, \concept[stable set problem]{stable set}, \concept[node cover problem]{node cover}) which implies that the best-known algorithms that guarantee an optimal solution have an \concept{enumerative character} (like the \concept{branch-and-price} approach).

\paragraph{}
This chapter deals with \concepts{approximation algorithm}. These are algorithms that return a \concept{feasible solution} fast (i.e. within \concept{polynomial time}), but sacrifice the \concept{guarantee of optimality}. Thus, whereas \concepts{branch-and-price algorithm} always output an \concept{optimum solution} at the expense of potentially enormous computing times, \concepts{approximation algorithm} form a \concept{dual approach} to solving \concepts{combinatorial optimization problem}. An \concept{approximation algorithm} guarantees a fast solution, but not necessarily an optimal one. Of course,
one still wants an \concept{approximation algorithm} to produce solutions that have a value that does not differ too much from the \concept{optimum value}. In order to be able to judge approximation algorithms, the concept of \concept{worst case analysis} is discussed in \secref{worstcase}. Further, we presents \concept{approximation algorithm} for two specific problems, namely \concept[node cover problem]{node cover} (\secref{nodecoverapprox}) and the \concept{traveling salesman problem} (\secref{tspapprox}).

\section{Worst case analysis}
\seclab{worstcase}

The \concept{worst case ratio} (WCR) of an algorithm $A$ for a \concept{minimization problem} $P$ is defined as follows:

\begin{definition}[Worst case ratio]
Given an \concept{instance} $I$ of \concept{problem} $P$, let the value of the \concept{solution} generated by \concept{algorithm} $A$ be $\fun{A}{I}$, and let the \concept{optimum value} be denoted by $\funm{opt}{I}$; we will sometimes simply write $\mbox{OPT}$, and ignore the ``$\fun{}{I}$''-part. The ratio

\begin{equation}
\fun{r}{I}\equiv\displaystyle\frac{\fun{A}{I}}{\funm{opt}{I}}
\end{equation}

is a \concept{measure} of the \concept{quality of the solution} found. (Notice that other \concept{measure} are certainly possible as well!). Now, the \concept{smallest upper bound} on $\fun{r}{I}$, measured over all \concepts{instance} $I$ of $P$ is called the \concept{worst case ratio} (WCR) of \concept{algorithm} $A$, i.e.,

\begin{equation}
\funm{wcr}{I}\equiv\fun{\sup_I}{\fun{r}{I}}=\fun{\sup_I}{\displaystyle\frac{\fun{A}{I}}{\funm{opt}{I}}}
\end{equation}

Notice that this \concept{ratio} is at least $1$. For a \concept{maximization problem} $P$ we define a similar \concept{quality measure} as
follows.

\begin{equation}
\funm{wcr}{I}\equiv\fun{\inf_I}{\fun{r}{I}}=\fun{\inf_I}{\displaystyle\frac{\fun{A}{I}}{\funm{opt}{I}}}
\end{equation}
This \concept{ratio} is at most $1$ (and not smaller than $0$).
\end{definition}

\paragraph{}
How can one find the \concept{worst case ratio} of a certain \concept{algorithm}? Almost always this amounts to applying a problem specific analysis. In such an analysis two things must
be argued:
\begin{enumerate}
 \item one has to argue that for all \concepts{instance} $I$ of $P$ it is true that $\dfrac{\fun{A}{I}}{\funm{opt}{I}}\leq R$, and that
 \item there exists an \concept{instance} $I$ of $P$ for which the ratio $\dfrac{\fun{A}{I}}{\funm{opt}{I}}$ is equal (or arbitrarily close) to $R$.
\end{enumerate}

Then one can conclude that $\funm{WCR}{A}=R$. Notice that the \concept{worst case ratio} depends on the \concept{algorithm}, thus different \concepts{algorithm} for the same problem can yield different \concepts{worst case ratio} (as we shall see in this chapter). Of course, one could ask the question: ``How well can we \concept{approximate} a certain \concept{problem} when using only \concept{polynomial time} algorithms?''. More formal, let $\funm{\textsc{PolyTime}}{P}=\condset{A}{\mbox{$A$ is a polynomial time algorithm for $P$}}$, then one is interested in something that can be formulated as:

\begin{equation}
\fun{\inf_{A\in\funm{\textsc{\small{PolyTime}}}{P}}}{\funm{wcr}{A}}
\end{equation}

This is certainly a valid question, and in recent years a number of results in this direction have been obtained; however, we will not go into this issue.

\section{Node Cover}
\seclab{nodecoverapprox}

Recall that given a \concept{graph} $G=\tupl{V,E}$, a node cover is a \concept{subset} of the \concept[vertex]{vertices} $W\subseteq V$ with the property that each \concept{edge} in $E$ is \concept{incident} to at least one \concept{vertex} in $W$. The \concept{objective} in this problem is to find a \concept{node cover} with a minimum number of \concepts{node}. This problem is \concept{NP-hard}. Let us now describe two \concepts{approximation algorithm} for node cover.

\importalgorithmicalgorithm{nodecover1}{First proposed approximating algorithm for the node cover problem.}
\importalgorithmicalgorithm{nodecover2}{Second proposed approximating algorithm for the node cover problem.}

\algoref{nodecover1} is a \concept[Greedy algorithm]{greedy type of algorithm}. At first glance it seems to be better than \algoref{nodecover2}, as it uses at least part of the structure of the \concept{graph}. And indeed, in instances arising from practical applications, it turns out that \algoref{nodecover1} outperforms \algoref{nodecover2} with respect to the number of \concepts{node} in the \concept[node cover]{cover} found. However, as we are about to discover, the WCR of \algoref{nodecover2} is much better than the WCR of \algoref{nodecover1}.

\paragraph{}
Consider the following instance depicted in \figref{nodecover-approx-example}.

\importtikzfigure{nodecover-approx-example}{An instance for node cover.}

What is the \concept{optimum}? It is not hard to figure out that one needs at least $5$ nodes for a \concept{node cover} in the instance above, and moreover, that taking all nodes from layer $B$ indeed constitutes a \concept{feasible node cover}. Thus, this is an \concept{optimum solution}. Let us now apply \algoref{nodecover1} to this instance and let us break ties by choosing nodes in the lowest possible layer first (that is nodes of layer $A$ enjoy priority over nodes in layer $B$ which enjoy priority over nodes in layer $C$). What happens? \algoref{nodecover1} selects first the nodes from layer $A$ and then the nodes from layer $B$ concluding with a node cover consisting of $8$ nodes. Even worse, we can generalize this instance as follows: let the number of nodes in layer $B$ be $n$, then there are also $n$ nodes in layer $C$ and there are at most $n-1$ nodes in the bottom layer. Again, each node in layer $A$ is connected to each node in layer $B$ and a node in layer $C$ is connected only to its ``\concept{companion node}'' directly beneath it. Thus the \concept{optimum node cover} consists of all nodes in layer $B$ with optimal value $n$, whereas \algoref{nodecover1} will find a solution consisting of all nodes of layer $A$ and $B$, with a value equal to almost twice the optimum value. What can we deduce from this example concerning the \concept{worst case ratio} of \algoref{nodecover1}? Well, we can only say that it is at least $2$. One cannot conclude that the \concept{worst case ratio} equals $2$ since there may exist instances on which \algoref{nodecover1} fares even worse.

\paragraph{}
Such examples exist. Consider \figref{nodecover-approx-exampld}. In this instance we see that the \concept{optimum node} cover still consists of all middle nodes (i.e. value $6$), whereas the \concept{node cover} constructed by \algoref{nodecover1} will consist of all nodes in the two bottom layers (i.e. a solution with value $13$). From this instance alone we can conclude that the worst case ratio of \algoref{nodecover1} must be worse than $2.1666$.

\importtikzfigure{nodecover-approx-exampld}{Another instance for node cover.}

\paragraph{}
By generalizing this instance an even more dramatic statement can be made:
\begin{theorem}
\thmlab{wcr-nodecover-approx1}
For each $r\geq 0$, there exist \concepts{instance} $I$ of \concept{node cover} such that $\fun{A}{I}\geq r\cdot\funm{opt}{I}$.
\end{theorem}
In other words, the \concept{worst case ratio} of \algoref{nodecover1} is \concept[unbounded worst case ratio]{unbounded}; it is impossible to find a constant $r$ such that
\begin{equation}
\displaystyle\frac{\fun{A}{I}}{\funm{opt}{I}}\leq r.
\end{equation}

\paragraph{}
Let us motivate this theorem by generalizing the instance in \figref{nodecover-approx-exampld}. The structure of this instance is as follows. In case of $6$ middle nodes we do the following: \concept{partition} the nodes from $B$ into $3$ pairs and join the nodes in each pair with a node from layer $A$. Then we partition the nodes in layer $B$ into two triples, and again join all nodes in a triple with a new node from layer $A$. Repeat this for quadruples and quintuples, and so on, possibly leaving out some nodes from layer $B$, and always adding a new node in layer $A$. Then, when applying \algoref{nodecover1}, there is always a node from the bottom layer with highest degree, consequently \algoref{nodecover1} will find a \concept{node cover} consisting of $\fun{L}{n}+n$ \concepts{node}, where $\fun{L}{n}$ is the number of \concepts{node} in layer $A$. How large is $\fun{L}{n}$? Observe that

\begin{equation}
\fun{L}{n}=\sumieqb[j]{2}{n-1}{\floor{\displaystyle\frac{n}{j}}}.
\end{equation}
We leave the exact proof of \thmref{wcr-nodecover-approx1} as an exercise.

What about \algoref{nodecover2}? It is easy to establish a \concept{lower bound} of $2$ on the \concept{worst case ratio}. Indeed, when simply taking a \concept{graph} consisting of $2\cdot n$ \concept[vertex]{vertices} and $n$ \concepts{edge} forming a \concept{perfect matching}, one observes that $n$ is the value of a \concept{minimum node cover}, whereas \algoref{nodecover2} selects all $2\cdot n$ nodes. However, it turns out that this is the worst that can happen for \algoref{nodecover2}:

\begin{theorem}
\thmlab{wcr-nodecover-approx2}
$\funm{wcr}{\algoref{nodecover1}}=2$.
\end{theorem}

The argument is as follows. Of course, any \concept{node cover} must cover all \concepts{edge} chosen by \algoref{nodecover2}. However, these \concepts{edge} do not have a \concept{node} in common, and therefore each \concept{edge} must be covered by a different node in any \concept{node cover}. Thus no \concept{node cover} can be smaller than half the size of the \concept{node cover} found by \algoref{nodecover2}. Together with the example sketched above \thmref{wcr-nodecover-approx2} now follows.

\section{The Traveling Salesman Problem (TSP)}
\seclab{tspapprox}
In a sense the \concept{TSP} is a harder problem to solve than \concept{node cover}. This follows from the well known fact that, unless $\cclass{P}=\cclass{NP}$, no polynomial time algorithm for the TSP exists that has a bounded \concept{WCR} (which contrasts with \algoref{nodecover2} in \secref{nodecoverapprox}). What we can do is to restrict our instances. Recall that the input to the \concept{TSP} is a \concept{distance matrix} $D$. In the sequel we restrict ourselves to instances for which the distances satisfy the \concept{triangle inequality}.

\begin{definition}[Triangle inequality]
The \concept{triangle inequality} is a restriction on a distance metric $\delta:X\times X\rightarrow\RRR$ that states that for every three items $x_1,x_2,x_3\in X$, the distance $\fun{\delta}{x_1,x_3}$ is less than or equal to the sum of the distance from $x_1$ to $x_2$ and the distance from $x_2$ to $x_3$. More formally:
\begin{equation}
\forall x_1,x_2,x_3\in X:\fun{\delta}{x_1,x_3}\leq\fun{\delta}{x_1,x_2}+\fun{\delta}{x_2,x_3}
\end{equation}
\end{definition}

Observe that many practical problems satisfy this restriction. Indeed, \concept{TSP} instances coming from actual ``\concept{travel settings}'' are quite likely to obey the \concept{triangle inequality}.

\subsection{The double tree algorithm (DT)}
\ssclab{doubletree}

The \concept{double tree algorithm} consists of four \concepts{phase}. In the first three \concepts{phase}, we construct an \concept{Eulerian cycle} that we convert into a \concept{Hamilton circuit} in \phsref{tsp-dt-d}:

\begin{phasenum}
 \item \phslab{tsp-dt-a} Construct a \concept{minimum spanning tree} with respect to the \concept{distance function} $d$.
 \item \phslab{tsp-dt-b} Double all \concepts{edge} in the \concept{tree}. Notice that the resulting graph is \concept[Eulerian graph]{Eulerian}.
 \item \phslab{tsp-dt-c} Determine an \concept{Eulerian cycle} in the \concept{Eulerian graph} determined in \phsrf{tsp-dt-b}. Since the \concept{Eulerian graph} is \concept[connected graph]{connected} (it contains the \concept{minimum spanning tree} as a subgraph), the \concept{cycle} contains each \concept{vertex} at least once.
 \item \phslab{tsp-dt-d} Convert the \concept{Eulerian cycle} into a \concept{Hamiltonian cycle} by applying \concepts{shortcut}, i.e., replace a pair of \concepts{consecutive edge} $\tupl{i,j}$ and $\tupl{j, k}$ in the \concept{Eulerian cycle} by $\tupl{i,k}$. We are only allowed to do this if $j$ appears somewhere else in the \concept{Eulerian cycle}.
\end{phasenum}

\begin{example}
Consider the $7$-\concept{city} \concept{TSP} instance depicted \tblref{tsp-approx-dst} and \figref{tsp-approx-ph1}.

\importtabulartable{tsp-approx-dst}{The distance matrix of the Traveling Salesman Problem example.}

\begin{figure}[hbt]
\centering
\importtikzsubfigure{tsp-approx-ph1}{Phase $1$.}
\importtikzsubfigure{tsp-approx-ph2}{Phase $2$.}
\caption{Phase $1$ and $2$ of the \concept{double tree algorithm}.}
\end{figure}
\end{example}

\paragraph{}
The \concept{Eulerian cycle} generated in \concept{phase} $3$ is $1-2-3-2-4-5-6-7-6-5-4-2-1$.

\paragraph{}
The operation described in \phsref{tsp-dt-d} can be performed on any \concept{cycle}. Its effect is that a new \concept{cycle} is constructed with one edge less; in this cycle, there is one \concept{vertex} that is visited one time less in comparison with the previous \concept{cycle}. Due to the assumption that the \concept{distance function} satisfies the \concept{triangle inequality}, it follows that applying a \concept{shortcut} does not increase the \concept[length of a cycle]{length of the cycle}. Let us formally record this observation in a lemma.

\begin{lemma}
Let $G=\tupl{V,E}$ be a \concept{complete graph}, and let $\funsig{d}{E}{\RRR^+}$ be a \concept{distance function} on the \concepts{edge}, which satisfies the \concept{triangle inequality}. Let $C$ be a \concept{cycle} in this \concept{graph} with total \concept[length of a cycle]{length} $\fun{d}{C}$. If $C'$ is a \concept{cycle} constructed from $C$ by applying \concepts{shortcut}, then we have that the total length of $C'$ is no more than $\fun{d}{C}$.
\end{lemma}

\paragraph{}
Let us now be more specific concerning \phsref{tsp-dt-d}. We apply a \concept{shortcut} on each second appearance of a \concept{vertex} $j$. Therefore, each \concept{vertex} remains in the \concept{cycle} at least once. After the \concept{shortcut} has been applied to all second appearances of the \concept[vertex]{vertices}, the \concept{cycle} contains each \concept{vertex} exactly once, that is, we have constructed a \concept{Hamiltonian cycle}.

\importtikzfigure{tsp-approx-ph4}{Phase $4$: replacing $3-2-4$ by $3-4$}

\importtikzfigure{tsp-approx-ph5}{Phase $4$: further replacements}

\paragraph{}
We now show that the \concept{double tree algorithm} will never produce a solution with length more than twice the length of an optimal solution. In other words:
\begin{theorem}
$\funm{wcr}{\mbox{double tree}}\leq2$.
\end{theorem}

Given an instance $I$, let $\funm{opt}{I}$ denote the length of an \concept{optimal tour}, and let $\fun{z_{dt}}{I}$ denote the length of the \concept{tour} constructed by \concept{double tree algorithm}. Finally, let $\funm{z_T}{I}$ denote the length of a \concept{minimum spanning tree}. We first prove that $\funm{z_T}{I}\leq\funm{opt}{I}$ for all instances $I$; 
\begin{proof}Consider any \concept{optimal tour}. If we delete an arbitrary \concept{edge} from it, then we obtain a... \concept{spanning tree}. By definition, the \concept[length of a spanning tree]{length} of a \concept{spanning tree} does not exceed the \concept{length of a minimal spanning tree}, and hence, we know that its length amounts to at least $\fun{z_T}{I}$. Concluding, we have that $\fun{z_T}{I}\leq\funm{opt}{I}$.\end{proof}
We then complete the proof by showing that zDT (I) ≤ 2 · zT (I):

2. The total length of the edges in the Eulerian graph that is constructed by doubling the minimum
spanning tree is equal to 2 · zT . From Lemma 6.3, it follows that the length zDT of the Hamiltonian
circuit that is obtained by applying shortcuts, amounts to no more than the length of the Eulerian
cycle, which is equal to 2 · zT .

Combining both results, we get zDT ≤ 2 · zT ≤ 2 · OPT.

\subsection{The tree-matching algorithm (TM)}
\ssclab{treematching}

From the analysis above, it follows that, if we want to improve our worst-case ratio, then we can try to
decrease the length of the Eulerian cycle. In the sequel we will do so. This means that we use the same
structure of algorithm DT. In fact, Phases 1, 3, and 4 in Algorithm TM are identical to the corresponding
phases in Algorithm DT. Thus, we only change the phase in which the Eulerian Cycle is constructed.
Recall that a graph is Eulerian if and only if it is connected, and each vertex has even degree. It seems
obvious to start with a minimum spanning tree to make sure that the graph is connected. The only
problem left is to take care of the vertices with odd degree, which we denote by V0 ; notice that the
number of vertices with odd degree is even. We see that we can get even degree in each of these vertices
by adding a perfect matching on these vertices V0 (see Chapter 1 for the definition of a perfect matching).
This is exactly what happens in phase 2 of algorithm TM, where we will compute a minimum weight
perfect matching M on the vertices in V0 .
Consider the example again. The initial minimum spanning tree contains 4 vertices of odd degree, namely
1, 2, 3, and 7. The minimum weight perfect matching of these vertices consists of the edges {1, 2} and
{3, 7}. Thus, we get the following extension of the minimum spanning tree.

Figure 6.6: Phases 2 of algorithm TM.

An Euler-cycle in this graph is 123765421, where 2 is the only vertex that appears more than once, and
therefore we remove one of its occurrences.
This small change with respect to algorithm DT results in a better worst-case behaviour. For any instance
of the TSP (satisfying the triangle-inequality), algorithm TM constructs a tour with length no more than
3
2

times the length of an optimal tour. To prove this, it suffices to show that the length zM of the

matching M is no more than

1
2

times OPT. Namely, then zT M ≤ zT + zM ≤ OPT + 12 · OPT =
81

3
2

· OPT.

Figure 6.7: Replacing 421 by 41.

The proof makes use of a shrinking argument. Consider any optimal tour with value OPT. Apply
shortcuts such that only the vertices in V0 remain in the cycle; this yields a cycle C on the vertices in
V0 with length no more than OPT. C can be partitioned into two perfect matchings M1 and M2 on V0
by ‘walking' along the circuit and putting the first edge in M1 , the second edge in M2 , the third edge in
M1 , etc. Notice that the cycle contains an even number of edges, because |V0 | is even.
Since M1 and M2 are perfect matchings on V0 , we have that d(M ) ≤ d(M1 ) and d(M ) ≤ d(M2 ). Hence,
we have that 2 × d(M ) ≤ d(M1 ) + d(M2 ) = d(C) ≤ OPT, which was to be proved.
Notice that for both algorithms, our analysis of the worst-case ratio depends heavily on the assumption
that the triangle inequality holds. It can be shown that, in case the triangle inequality fails to hold, the
worst-case ratio is unbounded (as could be inferred from the beginning of this section).

\section*{Exercises}
\begin{exercise}
Prove Theorem 6.1.
\end{exercise}

\begin{exercise}
Consider the following greedy algorithm for the knapsack problem. Sort the objects by decreasing ratio
of profit and size, and reindex the items such that the order of objects is 1, 2, . . . , n. Next, greedily pick
objects in this order while ensuring that the capacity of the knapsack is not exceeded.
• Show that this method can behave arbitrarily bad,
• Modify this method by identifying the smallest k such that the total size of the first k objects

exceeds the capacity. Next, find the best of the following two solutions: {1, 2, . . . , k − 1} and {k}.
Show that this algorithm is a 2-approximation.
\end{exercise}

\begin{exercise}
Consider the node packing problem. What can you tell about the WCR of each of the following two
algorithms?
Algorithm 1 (Input: a graph G = (V, E); output: a node packing W )
W = ∅, G′ = (V ′ , E ′ ) := G
while V ′ = ∅
do
Choose a node v ∈ V ′ with smallest degree;
W := W ∪ {v};
Update G′ , that is V ′ := V ′ \ {v} and remove from E ′ all edges incident to v;
od
Algorithm 2 (Input: a graph G = (V, E); output: a node packing W )
W = ∅, G′ = (V ′ , E ′ ) := G
while E ′ = ∅
do
Choose an arbitrary edge {v,w} in E ′ ;
W := W ∪ {v};
Update G′ , that is V ′ := V ′ \ {v, w} and remove from E ′ all edges incident to v or w;
od
\end{exercise}

\begin{exercise}
Consider the matching problem. What can you tell about the WCR of the following greedy algorithm
for matching?
Algorithm (Input: a graph G = (V, E); output: a matching M )
M = ∅, G′ = (V ′ , E ′ ) := G
while E ′ = ∅
do
Choose an arbitrary edge {v, w} ∈ E ′ ;
M := M ∪ {v, w};
Update G′ , that is V ′ := V ′ \ {v, w} and remove from E ′ all edges incident to v or w;
od
\end{exercise}

\begin{exercise}
An edge coloring of a graph is a coloring of the edges such that no two edges connected to the same
vertex have the same color. The edge coloring problem is to minimize the number of colors used to color
a graph. The greedy algorithm for edge coloring colors edge after edge. When coloring an edge it will
first consider colors that are already in use before assigning a new color. What can you say concerning
the WCR of this algorithm?
\end{exercise}

\begin{exercise}
Can you find instances of the TSP which imply that (together with Theorem 6.4) that WCR(DT)= 2?
\end{exercise}

\begin{exercise}
A TSP instance is called geometric if each of the cities can be represented by a point in the plane, i.e.
each city lies at coordinates (xi , yi ), i = 1, . . . , n. What do the result in the previous exercise tell you
about WCR(DT) for the geometric TSP?
\end{exercise}