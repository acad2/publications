\chapter{Approximation algorithms}
\chplab{approx}

\section{Introduction}
Instances of combinatorial optimization problems cannot always be solved to optimality within a reasonable amount of computing time. Indeed, some combinatorial problems are so-called NP-hard (e.g.
knapsack, stable set, node cover) which implies that the best-known algorithms that guarantee an optimal solution have an enumerative character (like the branch-and-price approach).
This chapter deals with approximation algorithms. These are algorithms that return a feasible solution
fast (i.e. within polynomial time), but sacrifice the guarantee of optimality. Thus, whereas branch-andprice algorithms always output an optimum solution at the expense of potentially enormous computing
times, approximation algorithms form a dual approach to solving combinatorial optimization problems.
An approximation algorithm guarantees a fast solution, but not necessarily an optimal one. Of course,
one still wants an approximation algorithm to produce solutions that have a value that does not differ
too much from the optimum value. In order to be able to judge approximation algorithms, the concept
of worst case analysis is discussed in Section 6.2. Further, we present approximation algorithms for two
specific problems, namely node cover (Section 6.3) and the TSP (Section 6.4).
73

\section{Worst case analysis}

The worst case ratio (WCR) of an algorithm A for a minimization problem P is defined as follows. Given
an instance I of problem P , let the value of the solution generated by algorithm A be A(I), and let the
optimum value be denoted by OPT(I); we will sometimes simply write OPT, and ignore the “(I)”-part.
The ratio
R(I) =

A(I)
OP T (I)

is a measure of the quality of the solution found. (Notice that other measure are certainly possible as
well!). Now, the smallest upper bound on R(I), measured over all instances I of P is called the worst
case ratio (WCR) of algorithm A, i.e.,
A(I)
.
OP T (I)

W CR(A) = supI

Notice that this ratio is at least 1. For a maximization problem P we define a similar quality measure as
follows.
W CR(A) = infI

A(I)
.
OP T (I)

This ratio is at most 1 (and not smaller than 0). How can one find the WCR of a certain algorithm?
Almost always this amounts to applying a problem specific analysis. In such an analysis two things must
be argued:
• one has to argue that for all instances I of P it is true that
• there exists an instance I of P for which the ratio

A(I)
OP T (I)

A(I)
OP T (I)

≤ R, and that

is equal (or arbitrarily close) to R.

Then one can conclude that W CR(A) = R. Notice that the WCR depends on the algorithm, thus different
algorithms for the same problem can yield different WCRs (as we shall see in this chapter). Of course, one
could ask the question: how well can we approximate a certain problem when using only polynomial time
algorithms. More formal, let P OLY T IM E(P ) = {A| A is a polynomial time algorithm for P }, then one
is interested in something that can be formulated as:
infA∈P OLY T IME(P ) W CR(A).
This is certainly a valid question, and in recent years a number of results in this direction have been
obtained; however, we will not go into this issue.

\section{Node Cover}

Recall that given a graph G = (V, E), a node cover is a subset of the vertices W ⊆ V with the property
that each edge in E is incident to at least one vertex in W . The objective in this problem is to find
a node cover with a minimum number of nodes. This problem is NP-hard. Let us now describe two
approximation algorithms for node cover.

Algorithm 1 (Input: a graph G = (V, E); output: a node cover W )
W = ∅, E ′ := E
while E ′ = ∅
do
Choose a node v ∈ V with largest degree;
W := W ∪ {v};
Update E ′ , that is remove from E ′ all edges incident to v;
od
Algorithm 2 (Input: a graph G = (V, E); output: a node cover W )
W = ∅, E ′ := E
while E ′ = ∅
do
Choose an arbitrary edge in E ′ ;
W := W ∪ {v, w};
Update E ′ , that is remove from E ′ all edges incident to v or w;
od
Algorithm 1 is a greedy type of algorithm. At first glance it seems to be better than Algorithm 2, as it uses
at least part of the structure of the graph. And indeed, in instances arising from practical applications,
it turns out that Algorithm 1 outperforms Algorithm 2 with respect to the number of nodes in the cover
found. However, as we are about to discover, the WCR of Algorithm 2 is much better than the WCR of
Algorithm 1.
Consider the following instance depicted in Figure 6.1.
75

C

①

B

①
①
①
①
①
✟
✑
✑
 
✁
❅
✁
 
 
❆
✁❆
✟
✑
✑
 ✁
❆❅
✁ ❆
✑ 
✑  ✁ ✟
❆   ✁ ✑   ✁✟✟
✑ 
❆❅ ✁
✁✑✑  
❆ ✁❅  ❆ ✁✑✑  ✟✟
✟
✁  
❆✁ ✟
  ✑
  ✑
❆✁ ❅
✑✁✟❆✟
 ✑✑✁  
✁ ❆  ✑❅
✟
✁  ❆✑✟ ✁❅ ❆✑ ✁  
✑✟  ✑
❅❆ ✁ 
✁ 
✑
✟✟ ❆ ✁✑
✁ 
❆①
✑
❅
✁
✑
✟
 
 
❆①
✁
①

A

①

①

①

①

Figure 6.1: An instance for node cover.

What is the optimum? It is not hard to figure out that one needs at least 5 nodes for a node cover in
the instance above, and moreover, that taking all nodes from layer B indeed constitutes a feasible node
cover. Thus, this is an optimum solution. Let us now apply algorithm 1 to this instance and let us break
ties by choosing nodes in the lowest possible layer first (that is nodes of layer A enjoy priority over nodes
in layer B which enjoy priority over nodes in layer C). What happens? Algorithm 1 selects first the nodes
from layer A and then the nodes from layer B concluding with a node cover consisting of 8 nodes. Even
worse, we can generalize this instance as follows: let the number of nodes in layer B be n, then there
are also n nodes in layer C and there are at most n − 1 nodes in the bottom layer. Again, each node in
layer A is connected to each node in layer B and a node in layer C is connected only to its “companion
node” directly beneath it. Thus the optimum node cover consists of all nodes in layer B with optimal
value n, whereas Algorithm 1 will find a solution consisting of all nodes of layer A and B, with a value
equal to almost twice the optimum value. What can we deduce from this example concerning the WCR
of Algorithm 1? Well, we can only say that it is at least 2. One cannot conclude that the WCR equals 2
since there may exist instances on which Algorithm 1 fares even worse.
And these examples exist. Consider Figure 6.2.
In this instance we see that the optimum node cover still consists of all middle nodes (i.e. value 6),
whereas the node cover constructed by Algorithm 1 will consist of all nodes in the two bottom layers (i.e.
a solution with value 13). From this instance alone we can conclude that the WCR of Algorithm 1 must
76

C

①

B

①
①
①
①
①
①
❛
❍
❛❛ ✁❍
❛❛ ✁❛
◗
◗
✑
◗
❆
❅
❅
✁
❆
❆
 
 
❍
◗
◗
✑
◗ ❛ ✁ ❅❅❍
❛❛
❍  ❆❅ ◗  ❅✑ ✁ ❆
✁ ❆ ◗
◗ ❛❛ ❍❍
❍❍❆ ❅
❛❆❛  ◗
❆
❛
❅❛✁ ❍
 ◗ ✑❅ ✁
◗
✁
❛
❍
❍
◗
✑
◗
❛ ❆❍ ❛◗
❆
✁❅
❆ ❍❅
 
✁
❛
◗
◗
✑
◗ ✁❅ ❛ 
❍
❍
❍ ◗
✁ ❅ ❆
❆❛❛❅
✑
◗
  ❛◗
  ❛❆❛❛❍
✁ ❅
✁
❍ ◗ ❅ ❆
❍
✑ ❆◗ ❛❅
 ◗❅ ❆  ❛
✁ ◗
✁
◗
◗ ❛✁❍
✑❛❍
❍
❛❍
❛
❛❍
❅ ❆
❛
✁❅
❆❍
◗❅ ❆✑
✁  
✁
❛◗
❛◗
❍
❍
◗
✑
◗
❛◗
❍
✁❍
❆ ❛
 
✁ 
✁
❛❅
❛❅
◗
◗
✑❅
◗❆
❍❆
❍
❍
❍
❆①
❛
◗
❛① ❛❅
◗
❆✁① ❛❅
❆①
✑
◗
❅
 
 
✁①
✁
①
①

A

①

①

①

①

①

Figure 6.2: Another instance for node cover.
be worse than 2.1666. However, by generalizing this instance an even more dramatic statement can be
made:
Theorem 6.1 For each r ≥ 0, there exist instances I of node cover such that A(I) ≥ r · OP T (I)
In other words, the WCR of Algorithm 1 is unbounded; it is impossible to find a constant R such that
A(I)
OP T (I)

≤ R. Let us motivate this theorem by generalizing the instance in Figure 6.2. The structure of

this instance is as follows. In case of 6 middle nodes we do the following: partition the nodes from B into
3 pairs and join the nodes in each pair with a node from layer A. Then we partition the nodes in layer
B into two triples, and again join all nodes in a triple with a new node from layer A. Repeat this for
quadruples and quintuples, and so on, possibly leaving out some nodes from layer B, and always adding
a new node in layer A. Then, when applying Algorithm 1, there is always a node from the bottom layer
with highest degree, consequently Algorithm 1 will find a node cover consisting of L(n) + n nodes, where
L(n) is the number of nodes in layer A. How large is L(n)? Observe that L(n) =

n−1
j=2 ⌊n/j⌋.

We leave

the exact proof of Theorem 6.1 as an exercise.
What about Algorithm 2? It is easy to establish a lower bound of 2 on the WCR. Indeed, when simply
taking a graph consisting of 2n vertices and n edges forming a perfect matching, one observes that n is
the value of a minimum node cover, whereas Algorithm 2 selects all 2n nodes. However, it turns out that
this is the worst that can happen for Algorithm 2:
77

Theorem 6.2 W CR(Algorithm2) = 2.
The argument is as follows. Of course, any node cover must cover all edges chosen by Algorithm 2.
However, these edges do not have a node in common, and therefore each edge must be covered by a
different node in any node cover. Thus no node cover can be smaller than half the size of the node cover
found by Algorithm 2. Together with the example sketched above Theorem 6.2 now follows.

\section{The Traveling Salesman Problem (TSP)}

In a sense the TSP is a harder problem to solve than node cover. This follows from the well known fact
that, unless P=NP, no polynomial time algorithm for the TSP exists that has a bounded WCR (which
contrasts with Algorithm 2 in the previous section).
What we can do is to restrict our instances. Recall that the input to the TSP is a distance matrix D. In
the sequel we restrict ourselves to instances for which the distances satisfy the triangle inequality, that is,
we have that dik ≤ dij + djk for all i, j, k. Observe that many practical problems satisfy this restriction.
Indeed, TSP instances coming from actual “travel settings” are quite likely to obey the triangle inequality.
The double tree algorithm (DT).
Algorithm DT consists of four phases. In the first three phases, we construct an Euler-cycle that we
convert into a Hamilton circuit in Phase 4.
Phase 1. Construct a minimum spanning tree with respect to the distance function d.
Phase 2. Double all edges in the tree. Notice that the resulting graph is Eulerian.
Phase 3. Determine an Euler cycle in the Eulerian graph determined in Phase 2. Since the Eulerian
graph is connected (it contains the minimum spanning tree as a subgraph), the cycle contains each vertex
at least once.

Phase 4. Convert the Euler cycle into a Hamilton cycle by applying shortcuts, i.e., replace a pair of
consecutive edges {i, j} and {j, k} in the Euler cycle by {i, k}. We are only allowed to do this if j appears
somewhere else in the Euler cycle.
Example Consider the following 7-city TSP instance.
78

1
1

2

3

4

5

6

7

1

2

2

3

4

5

1

1

3

3

4

2

3

2

3

1

2

3

1

4

2
3
4
5

1

6
7
3 ✉

3 ✉
2 ✉

4
✉

5
✉

6
✉

7
✉

2 ✉

4
✉

5
✉

6
✉

7
✉

1 ✉

1 ✉

Figure 6.3: Phases 1 and 2.
The Euler-cycle generated in phase 3 is 1232456765421.
The operation described in Phase 4 can be performed on any cycle. Its effect is that a new cycle is
constructed with one edge less; in this cycle, there is one vertex that is visited one time less in comparison
with the previous cycle. Due to the assumption that the length function satisfies the triangle inequality,
it follows that applying a shortcut does not increase the length of the cycle. Let us formally record this
observation in a lemma.
Lemma 6.3 Let G = (V, E) be a complete graph, and let d : E → R+ be a distance function on the
edges, which satisfies the triangle inequality. Let C be a cycle in this graph with total length d(C). If C ′
is a cycle constructed from C by applying shortcuts, then we have that the total length of C ′ is no more
than d(C).
Let us now be more specific concerning Phase 4. We apply a shortcut on each second appearance of a
vertex j. Therefore, each vertex remains in the cycle at least once. After the shortcut has been applied
to all second appearances of the vertices, the cycle contains each vertex exactly once, that is, we have
constructed a Hamilton cycle.
79

3 ✉
2 ✉

4
✉

5
✉

6
✉

3 ✉
❅
❅
❅ 4
❅✉
2 ✉

7
✉

1 ✉

3 ✉
❅
❅
❅ 4
❅✉
2 ✉

5
✉

6
✉

7
✉

5
✉

6
✉

7
✉

1 ✉

5
✉

Figure 6.4: Phase 4: replacing 324 by 34
3 ✉
❅
❅
❅ 4
6
7
✉
✉
❅✉
2 ✉

1 ✉

1 ✉
Figure 6.5: Phase 4: further replacements

We now show that Algorithm DT will never produce a solution with length more than twice the length
of an optimal solution. In other words:
Theorem 6.4 W CR(DT ) ≤ 2.
Given an instance I, let OPT(I) denote the length of an optimal tour, and let zDT (I) denote the length
of the tour constructed by algorithm DT. Finally, let zT (I) denote the length of a minimum spanning
tree. We first prove that zT (I) ≤ OPT(I) for all instances I; we then complete the proof by showing
that zDT (I) ≤ 2 · zT (I).
1. Consider any optimal tour. If we delete an arbitrary edge from it, then we obtain a . . . spanning tree.
By definition, the length of a spanning tree does not exceed the length of a minimal spanning tree,
and hence, we know that its length amounts to at least zT . Concluding, we have that zT ≤ OPT.
2. The total length of the edges in the Eulerian graph that is constructed by doubling the minimum
spanning tree is equal to 2 · zT . From Lemma 6.3, it follows that the length zDT of the Hamiltonian
circuit that is obtained by applying shortcuts, amounts to no more than the length of the Eulerian
cycle, which is equal to 2 · zT .
80

Combining both results, we get zDT ≤ 2 · zT ≤ 2 · OPT.
The tree-matching algorithm (TM).
From the analysis above, it follows that, if we want to improve our worst-case ratio, then we can try to
decrease the length of the Eulerian cycle. In the sequel we will do so. This means that we use the same
structure of algorithm DT. In fact, Phases 1, 3, and 4 in Algorithm TM are identical to the corresponding
phases in Algorithm DT. Thus, we only change the phase in which the Eulerian Cycle is constructed.
Recall that a graph is Eulerian if and only if it is connected, and each vertex has even degree. It seems
obvious to start with a minimum spanning tree to make sure that the graph is connected. The only
problem left is to take care of the vertices with odd degree, which we denote by V0 ; notice that the
number of vertices with odd degree is even. We see that we can get even degree in each of these vertices
by adding a perfect matching on these vertices V0 (see Chapter 1 for the definition of a perfect matching).
This is exactly what happens in phase 2 of algorithm TM, where we will compute a minimum weight
perfect matching M on the vertices in V0 .
Consider the example again. The initial minimum spanning tree contains 4 vertices of odd degree, namely
1, 2, 3, and 7. The minimum weight perfect matching of these vertices consists of the edges {1, 2} and
{3, 7}. Thus, we get the following extension of the minimum spanning tree.
3 ✉
2 ✉

4
✉

5
✉

6
✉

✉❳
3 ❳
❳❳❳
❳❳
❳❳❳
❳❳❳
4
5
6 ❳❳ 7
❳✉
✉
✉
✉
✉
2

7
✉

1 ✉

1 ✉
Figure 6.6: Phases 2 of algorithm TM.

An Euler-cycle in this graph is 123765421, where 2 is the only vertex that appears more than once, and
therefore we remove one of its occurrences.
This small change with respect to algorithm DT results in a better worst-case behaviour. For any instance
of the TSP (satisfying the triangle-inequality), algorithm TM constructs a tour with length no more than
3
2

times the length of an optimal tour. To prove this, it suffices to show that the length zM of the

matching M is no more than

1
2

times OPT. Namely, then zT M ≤ zT + zM ≤ OPT + 12 · OPT =
81

3
2

· OPT.

❳❳❳
3 ✉
❳❳
❳❳❳
❳❳
❳❳❳
4
5
6 ❳❳ 7
✉
✉
✉
✉
❳✉
2

✉
3 ❳
❳❳❳
❳❳
❳❳❳
❳❳❳
4
5
6❳❳❳ 7
✉
✉
✉
❳✉
2 ✉
 
 
 
✉
1  

1 ✉

Figure 6.7: Replacing 421 by 41.
The proof makes use of a shrinking argument. Consider any optimal tour with value OPT. Apply
shortcuts such that only the vertices in V0 remain in the cycle; this yields a cycle C on the vertices in
V0 with length no more than OPT. C can be partitioned into two perfect matchings M1 and M2 on V0
by ‘walking' along the circuit and putting the first edge in M1 , the second edge in M2 , the third edge in
M1 , etc. Notice that the cycle contains an even number of edges, because |V0 | is even.
Since M1 and M2 are perfect matchings on V0 , we have that d(M ) ≤ d(M1 ) and d(M ) ≤ d(M2 ). Hence,
we have that 2 × d(M ) ≤ d(M1 ) + d(M2 ) = d(C) ≤ OPT, which was to be proved.
Notice that for both algorithms, our analysis of the worst-case ratio depends heavily on the assumption
that the triangle inequality holds. It can be shown that, in case the triangle inequality fails to hold, the
worst-case ratio is unbounded (as could be inferred from the beginning of this section).

Exercises
Exercise 1
Prove Theorem 6.1.
Exercise 2
Consider the following greedy algorithm for the knapsack problem. Sort the objects by decreasing ratio
of profit and size, and reindex the items such that the order of objects is 1, 2, . . . , n. Next, greedily pick
objects in this order while ensuring that the capacity of the knapsack is not exceeded.
• Show that this method can behave arbitrarily bad,
• Modify this method by identifying the smallest k such that the total size of the first k objects
82

exceeds the capacity. Next, find the best of the following two solutions: {1, 2, . . . , k − 1} and {k}.
Show that this algorithm is a 2-approximation.

Exercise 3
Consider the node packing problem. What can you tell about the WCR of each of the following two
algorithms?
Algorithm 1 (Input: a graph G = (V, E); output: a node packing W )
W = ∅, G′ = (V ′ , E ′ ) := G
while V ′ = ∅
do
Choose a node v ∈ V ′ with smallest degree;
W := W ∪ {v};
Update G′ , that is V ′ := V ′ \ {v} and remove from E ′ all edges incident to v;
od
Algorithm 2 (Input: a graph G = (V, E); output: a node packing W )
W = ∅, G′ = (V ′ , E ′ ) := G
while E ′ = ∅
do
Choose an arbitrary edge {v,w} in E ′ ;
W := W ∪ {v};
Update G′ , that is V ′ := V ′ \ {v, w} and remove from E ′ all edges incident to v or w;
od
Exercise 4
Consider the matching problem. What can you tell about the WCR of the following greedy algorithm
for matching?
Algorithm (Input: a graph G = (V, E); output: a matching M )
M = ∅, G′ = (V ′ , E ′ ) := G
while E ′ = ∅
do
Choose an arbitrary edge {v, w} ∈ E ′ ;
83

M := M ∪ {v, w};
Update G′ , that is V ′ := V ′ \ {v, w} and remove from E ′ all edges incident to v or w;
od
Exercise 5
An edge coloring of a graph is a coloring of the edges such that no two edges connected to the same
vertex have the same color. The edge coloring problem is to minimize the number of colors used to color
a graph. The greedy algorithm for edge coloring colors edge after edge. When coloring an edge it will
first consider colors that are already in use before assigning a new color. What can you say concerning
the WCR of this algorithm?
Exercise 6
Can you find instances of the TSP which imply that (together with Theorem 6.4) that WCR(DT)= 2?
Exercise 7
A TSP instance is called geometric if each of the cities can be represented by a point in the plane, i.e.
each city lies at coordinates (xi , yi ), i = 1, . . . , n. What do the result in the previous exercise tell you
about WCR(DT) for the geometric TSP?