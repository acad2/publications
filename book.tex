\documentclass[titlepage]{book}

\usepackage{fullpage,importsreferences-en,brackets-en,amsmath,amsthm,amssymb,amsfonts,tikz}
\usetikzlibrary{shapes}

\title{Optimization: Special Topics}
\author{Frits Spieksma\and Edited by: Jo Devriendt \and Willem Van Onsem}
\date{February, 2013\\Edited: February 2014}

\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[chapter]
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{exercise}{Exercise}[chapter]
\newtheorem{definition}{Definition}[chapter]
\theoremstyle{remark}
\newtheorem{example}{Example}
\newtheorem{application}{Application}

\begin{document}
\begin{titlepage}
\maketitle
\end{titlepage}
\tableofcontents
\chapter*{Preface}
These notes are lecture notes used for the course ``Optimization: special topics''. Its subject is mainly combinatorial optimization with an emphasis on modeling issues and solution strategies. Its audience is students, more specifically: master students in engineering or management or economic programs.

\paragraph{}
The course assumes that the reader has had a first introduction to operations research and has some elementary knowledge of mathematical modeling, linear programming, and graph theory.

\paragraph{}
There are some excellent textbooks on combinatorial optimization. We mention Schrijver's trilogy: ``Combinatorial Optimization. Polyhedra and Efficiency'' \cite{schrijver-book}. Another example is: ``Combinatorial optimization'' by Cook, Cunningham, Pulleyblank, and Schrijver \cite{Cook:98}. Other related textbooks are: Nemhauser and Wolsey \cite{citeulike:2212037}, Wolsey \cite{wolseyip}.

\paragraph{}
Please notice that Chapter 3 is written by Prof. Yves Crama.

\chapter{Formulations in combinatorial optimization}
In this first chapter, we will give an informal notion of combinatorial optimization problems. The defining characteristic of a combinatorial optimization problem is that it has a finite number of feasible solutions. We now give formulations of a number of basic combinatorial optimization problems.

\section{Formulations of basic combinatorial optimization problems}
\seclab{formulations}
Below, we present mathematical formulations of some well-known combinatorial optimization problems. Some of these problems can be formulated in a straightforward way. Others, however, have more complicated formulations. Among the latter ones are problems that can be solved very easily by a greedy algorithm, like the spanning tree problem. Other problems are notoriously hard to solve. This already shows that a formulation need not give insight into the problem's difficulty. Moreover, a problem may have several correct formulations. This leads us to a very important question: which formulations are good and which are bad? Clearly, an answer to this question depends on one's goal; we will adopt here a solver's point of view. Thus, a formulation is better than another formulation when it leads to better solutions, or when it produces solutions faster.

\paragraph{}
Almost all formulations will use binary or integral variables. This shows that integer programming is closely related to (or, as a matter of fact, is itself a problem from the domain of) combinatorial optimization.

\subsection{The Matching Problem}
\ssclab{matching}
The Matching Problem (also known as Edge Packing) is one of the fundamental problems in Combinatorial Optimization. It is described as follows. We are given an arbitrary graph $G=\tupl{V,E}$. A subset of the edges $E'\subseteq E$ is called a matching (or an edge packing) if each vertex of $V$ is incident to at most one edge of $E'$. In other words, $E'$ is a matching if no two edges of $E'$ have a vertex in common. The problem is to find a matching in G consisting of as many edges as possible (a maximum cardinality matching).

\importtikzfigure{matching-red}{The red edges form a matching.}

\paragraph{}
A matching is called maximum when no matching of larger cardinality exists. A matching is called maximal when it cannot be enlarged.

\paragraph{}
The matching problem can be formulated as an integer program as follows, where we use the symbol $\fun{\delta}{v}$ to denote the set of edges that are incident to vertex $v\in V$. For instance, in \figref{matching-red}, $\fun{\delta}{v}=\accl{\tupl{3,7},\tupl{4,7},\tupl{7,8}}$. We define a 0-1 variable for each edge $e\in E$ as follows:
\begin{equation}
\semboolvar{x_e}{if edge $e$ is selected in the matching;}{otherwise.}
\end{equation}
And here is the integer program:
\begin{eqnarray}
\mbox{maximize}&\sumdomain[e]{E}{x_e}\eqnlab{matching-m}\\
\mbox{subject to}&\forall v\in V:\sumdomain[e]{\fun{\delta}{v}}{x_e}\leq 1\eqnlab{matching-c1}\\
&\forall e\in E:x_e\in\accl{0,1}\eqnlab{matching-c2}
\end{eqnarray}

\paragraph{}
The weighted matching problem is a generalization of the cardinality matching problem above by assuming that there is a given weight function w defined on the edges. Then, the objective is to find a maximum weight matching, and the objective is changed accordingly to $\max\isumdomain[e]{E}{w_e\cdot x_e}$ (Obviously, it is a generalization, since the cardinality matching problem arises when $w_e$ for each $e\in E$). In the perfect matching problem the set of feasible solutions is restricted to perfect matchings. These are matchings such that each vertex is incident to precisely one edge in the matching; then, constraints \eqnref{matching-c1} become equalities. Notice that a perfect matching may not exist (consider e.g. a triangle), whereas there is always a feasible solution to \eqnref{matching-m}-\eqnref{matching-c2}.

\paragraph{}
We emphasize that we distinguish between, on the one hand, the problem itself, and, on the other hand, its formulation as an integer program. Indeed, these two are not the same! In fact, in some cases it is appropriate to show the correctness of a formulation. Such an argument is usually based on a correspondence between feasible solutions to the problem, and vectors of decision variables satisfying the constraints.

\paragraph{}
Let us illustrate this matter for the matching problem. Notice that there is a 1-1 correspondence between subsets of the set of edges $E$ and the 0-1 vectors defined by the variables, which are indexed by the edges. To prove that this formulation is correct we must show that there is a 1-1 correspondence between the subsets of the edges that define matchings and the feasible solutions of the above formulation. Moreover, we must show that the matching and its corresponding vector have the same value. This can be done as follows. First, consider an arbitrary matching $M$ in a graph. By definition, this implies that each node $v\in V$ of the graph is incident with at most one edge of $M$ . Let us now construct a solution vector $\vec{x}_M$ in a straightforward manner: we put a ``1'' in $\vec{x}_M$ when the corresponding edge is in $M$, and otherwise we put a ``0'' in the vector $\vec{x}_M$. Clearly, $\vec{x}_M$ is a 0-1 vector, and obviously satisfies \eqnref{matching-c2}. Also, the solution vector $\vec{x}_M$ corresponding to $M$ satisfies the constraints \eqnref{matching-c1} since at most one of the variables in the left-hand side has value 1. Thus, a matching $M$ corresponds to a feasible solution of the integer program. Second, consider a subset of the edges $E'$ that is \textbf{not} a matching. Then there is a vertex, say $v$, that is incident to at least two edges in $E'$. But then, at least two of the variables in the left-hand side of \eqnref{matching-c1} have value 1 for this particular vertex. Thus, the vector corresponding to $E'$ is not feasible in the formulation. Finally, we observe that the value of each set $E'\subseteq E$ is equal to its number of edges, i.e., $\abs{E'}=\isumdomain[e]{E'}{1}=\isumdomain[e]{E}{x_e}$.

\paragraph{}
In general, it may not be trivial to prove the 1-1 correspondence of feasible solutions of a combinatorial problem to solutions of its formulation, i.e., solutions satisfying the constraints. For many problems, however, a correctness proof is omitted because the problem is an (extension of) a well-known problem, and the correctness of a formulation is evident.

\subsection{The Independent Set Problem}
\ssclab{independentset}
Another basic problem within the field of combinatorial optimization is the Independent Set problem (also known as Stable Set, or as Node Packing). It can be described as follows.
\paragraph{}
Consider an arbitrary graph $G=\tupl{V,E}$. A subset of the vertices $V'\subseteq V$ is called an independent set (or a stable set, or a node packing) if each edge of $E$ is incident to at most one vertex of $V'$. In other words, $V'$ is an independent set if no two vertices of an edge are selected both. The problem is to find an independent set in $G$ containing as many vertices as possible (a maximum cardinality independent set). The problem can be formulated as an integer program as follows. We define a 0-1 variable for each edge $v\in V$ as follows:
\begin{equation}
\semboolvar{x_v}{if vertex $v$ is selected in the independent set;}{otherwise.}
\end{equation}
And here is the integer program:
\begin{eqnarray}
\mbox{maximize}&\sumdomain[v]{V}{x_v}\eqnlab{stableset-m}\\
\mbox{subject to}&\forall\tupl{v_1,v_2}\in E:x_{v_1}+x_{v_2}\leq 1\eqnlab{stableset-c1}\\
&\forall v\in V:x_v\in\accl{0,1}\eqnlab{stableset-c2}
\end{eqnarray}

\importtikzfigure{stableset-red}{A stable set.}

\subsection{Spanning forest}
\ssclab{spanningforest}
Consider an undirected graph $G=\tupl{V,E}$. A subset of the edges $E'\subseteq E$ is called a forest if the subgraph of $G$ induced by $E'$ is acyclic, see \figref{forest-red} for an example. In case a forest consists of $\abs{V}-1$ edges it is called a tree. The problem is to find a maximum weight forest in $G$. The problem to find a maximum weight forest can be formulated as an integer program as follows. We define a 0-1 variable for each edge $e\in E$ as follows:
\begin{equation}
\semboolvar{x_e}{if edge $e$ is selected in the forest;}{otherwise.}
\end{equation}
And here is the integer program:
\begin{eqnarray}
\mbox{maximize}&\sumdomain[e]{E}{w_e\cdot x_e}\eqnlab{forest-m}\\
\mbox{subject to}&\forall V'\subseteq V:2\leq\abs{V'}\leq\abs{V}:\sumdomain[e]{\fun{\delta}{v_1}\cap\fun{\delta}{v_2},v_1,v_2\in V}{x_e}\leq\abs{V'}-1\eqnlab{forest-c1}\\
&\forall e\in E:x_e\in\accl{0,1}\eqnlab{forest-c2}
\end{eqnarray}

\importtikzfigure{forest-red}{The edges in red form a forest.}
\paragraph{}The \eqncsref{forest-c1} limit the number of chosen edges with both endvertices in any given subset $V'$ of the nodes to at most $\abs{V'}-1$. Since a cycle contains as many edges as vertices, the \eqncsref{forest-c1} prevent the graph $\tupl{V,E'}$ from containing cycles.
\paragraph{}
Notice that the number of subsets of $V$ of size $2$ or bigger is $2^{\abs{V}}-\abs{V}-1$. Thus, the number of \eqnref{forest-c1} is exponential in the size of the problem. Thus, the size of the formulation is exponential in the size of the input, although the problem is trivially solvable by a greedy algorithm.
\paragraph{}
The spanning tree problem is a variant of the spanning forest problem, where the set of feasible solutions is restricted to those acyclic subgraphs that are connected. It is well-known that for an acyclic subgraph the requirement of connectedness is equivalent to the requirement of having $\abs{V}-1$ edges, i.e., $\abs{E'}=\abs{V}-1$. Thus, by adding the constraint $\isumdomain[e]{E}{x_e}=n-1$ to \eqnnrefr{forest-c1}{forest-c2}, a correct formulation of the minimum weight spanning tree problem arises.

\subsection{The Knapsack Problem}
\ssclab{knapsack}
We are given a set of $n$ items, each with a weight $a_j$ and a value $c_j$ for $\rangei[j]{1}{n}$. Feasible solutions are the subsets of the set of items with cumulative weight at most $b$. The objective is to find a feasible solution of maximum value. The problem formulation contains binary variables $x_j$ which indicate whether element $j$ with $\rangei[j]{1}{n}$ is in the knapsack:

\begin{equation}
\semboolvar{x_j}{if element $j$ is selected in the knapsack;}{otherwise.}
\end{equation}

And here is the integer program:

\begin{eqnarray}
\mbox{maximize}&\sumieqb[j]{1}{n}{c_j\cdot x_j}\eqnlab{knapsack-m}\\
\mbox{subject to}&\sumieqb[j]{1}{n}{a_j\cdot x_j}\leq b\eqnlab{knapsack-c1}\\
&\forall\rangei[j]{1}{n}:x_j\in\accl{0,1}\eqnlab{knapsack-c2}
\end{eqnarray}
In this text, we will come back extensively to the knapsack problem. There is a book devoted to this problem, see Kellerer et al.\cite{KelPfePis04}.

\section{Formulations and difficulty}
\ssclab{difficulty}
Does the formulation of a problem tell us anything about the problem's difficulty? The answer is no, it doesn't. Consider for instance the matching problem (\sscref{matching}) and the stable set problem (\sscref{stableset}). These problems look similar, since the roles of the edges and vertices are interchanged. However, there is a striking difference between them. The matching problem can be solved in polynomial time, but the node packing problem is NP-hard (see \chpref{complexity}). In other words, whereas, for the matching problem, fast and efficient algorithms exist, and have been designed, no such algorithms have been found for the node packing problem. Indeed, this does not rule out the possibility that fast algorithms could exist for node packing, however, no one has ever found such an algorithm. In fact, it is widely suspected that such algorithms do not exist, but a proof of this hypothesis is lacking. All this boils down to the famous, 1 million-dollar worth, P = NP question. In practice, this means that we can find an optimal solution to a matching problem on a graph with, say 10.000 nodes and 50.000 edges within seconds, while there is no algorithm known that would return an optimal solution with one hour for the stable set instance on the same graph. Therefore: a formulation does not give an indication of the solvability of a problem. Also, the number of variables and/or constraints is no clue concerning the difficulty of a problem. For instance, the ``natural'' formulation of the minimum spanning tree problem (see \sscref{spanningforest}) has an exponential number of constraints, while the problem is easily solvable by a greedy algorithm.

\section{Multiple formulations of a combinatorial optimization problem}
\seclab{multipleformulations}

\subsection{Traveling Salesman Problem}
\ssclab{tsp}
Probably the most well-known problem in Combinatorial Optimization is the Traveling Salesman Problem (TSP). The TSP is the prototype combinatorial optimization problem. No other problem has received as much attention as the TSP, and no other problem has captured the imagination as an easy-to-describe, yet hard-to-solve problem. A description of the problem is as follows. Given is a set of $n$ ``cities'' and a distance $c_{i,j}$ between each pair of them. The goal is to find a tour of minimum length, that is to start in some city, visit each other city once, and to return to the city where the tour was started. More formally, given an $n\times n$ matrix $C=c_{i,j}$, find a permutation $\pi$ of $\accl{1,2,\ldots,n}$ such that $c_{\fun{\pi}{n},\fun{\pi}{1}}+\isumieqb[i]{1}{n-1}{c_{\fun{\pi}{i},\fun{\pi}{i+1}}}$ is minimum.
Different formulations of the TSP exist. Here is a conventional one, using binary variables $x_{i,j}$ indicating whether city $j$ is visited directly after city $i$:

\begin{eqnarray}
\mbox{minimize}&\sumieqb[i]{1}{n}{\sumieqb[j]{1}{n}{c_{i,j}\cdot x_{i,j}}}\eqnlab{tsp-m}\\
\mbox{subject to}&\forall\rangei[j]{1}{n}:\sumieqb[i]{1}{n}{x_{i,j}}=1\eqnlab{tsp-c1}\\
&\forall\rangei[i]{1}{n}:\sumieqb[j]{1}{n}{x_{i,j}}=1\eqnlab{tsp-c2}\\
&\forall S\subsetneq V:2\leq\abs{S}:\sumdomain[i]{S}{\sumdomain[j]{S}{x_{i,j}}}\leq\abs{S}-1\eqnlab{tsp-c3}\\
&\forall\rangei[i,j]{1}{n}:x_{i,j}\in\accl{0,1}\eqnlab{tsp-c4}
\end{eqnarray}

\paragraph{}
\eqncsref{tsp-c3} are called the subtour elimination constraints. An equivalent way of writing them is:
\begin{equation}
\forall S\subsetneq V:\abs{S}\geq 2:\sumdomain[i]{S}{\sumndomain[j]{S}{x_{i,j}}}\geq 1
\end{equation}

\paragraph{}
Notice that this formulation has a polynomial number of variables ($n^2$), and an exponential number of constraints (\bigoh{2^n}). The latter fact might be considered a disadvantage of formulation \eqnnrefr{tsp-m}{tsp-c4}.

\paragraph{}
There is, however, an alternative for this formulation which uses additional real variables $u_i$ , one such variable for each city $i$. The interpretation of this variable is the position of city $i$ in the tour, while putting the position of city $1$ first. Next, by replacing \eqncsref{tsp-c3} by the following constraints:

\begin{eqnarray}
\forall \rangei[i,j]{2}{n}:i\neq j:u_i-u_j+n\cdot x_{i,j}\leq n-1\eqnlab{mtz-c1}\\
u_1=1\eqnlab{mtz-c2}
\end{eqnarray}

we arrive at a formulation that is called the Miller-Tucker-Zemlin (MTZ) formulation of the TSP. It is an interesting exercise to verify the correctness of the MTZ-formulation. More concrete: why is it that \eqnnref{mtz-c1} exclude subtours? The answer lies in noticing that: any solution satisfying \eqnnref{tsp-c1}, \eqnnref{tsp-c2}, and \eqnnref{tsp-c4} consists of a collection of subtours (or a feasible solution, that is, a single tour). In case there are subtours, then there is a subtour that does not contain city 1. Let us now, for each pair of consecutive cities $i$ and $j$ in this subtour, sum the corresponding inequalities \eqnnref{mtz-c1}. The $u$-variables will cancel out, and the resulting lefthand side will be larger than the resulting righthand side, which means that this subtour will be forbidden by \eqnnref{mtz-c1}. Thus, any subtour not containing city 1 will be forbidden, and hence, the only possible solution is a single tour.

\paragraph{}
There is more than one book devoted to the TSP. We mention: Applegate et al. \cite{Applegate.2006} and Lawler et al. \cite{Lawler85}.

\paragraph{}
We will end this section with a problem for which we have two natural formulations. Both formulations have the same sets of variables, but they have different sets of constraints. Later we will see different formulations of problems where the sets of variables differ.

\subsection{Uncapacitated Facility Location}
\ssclab{facilitylocation}

We are given a set of $m$ facilities and $n$ clients. Let us call the set of facilities $M\equiv\accl{1,2,\ldots,m}$, and let us call the set of clients $N\equiv\accl{1,2,\ldots,n}$. Each of the facilities can (but need not be) be opened to serve clients. Each client must be served by a facility. The cost for opening facility $i$ is $f_i$, $i\in M$; the cost for serving client $j$ by facility $i$ is $c_{i,j}$, $i\in M$, $j\in N$. This problem can be formulated in two ways with the following sets of variables, defined for each $i\in M$, $j\in N$:

\begin{eqnarray}
\semboolvar{x_{i,j}}{if facility $j$ serves client $j$;}{otherwise.}\\
\semboolvar{y_i}{if facility $i$ is open;}{otherwise.}
\end{eqnarray}

\paragraph{}
The first formulation makes use of the fact that there is an upper bound on the number of clients that are served by a facility, namely the total number of clients $n$.

\begin{eqnarray}
\mbox{minimize}&\sumdomain[i]{M}{\brak{f_i\cdot y_i+\sumdomain[j]{N}{c_{i,j}\cdot x_{i,j}}}}\eqnlab{ufl-ma}\\
\mbox{subject to}&\forall j\in N:\sumdomain[i]{M}{x_{i,j}}=1\eqnlab{ufl-ca1}\\
&\forall i\in M:\sumdomain[j]{N}{x_{i,j}}\leq n\cdot y_i\eqnlab{ufl-ca2}\\
&\forall i\in M,j\in N:x_{i,j},y_i\in\accl{0,1}\eqnlab{ufl-ca3}
\end{eqnarray}

\paragraph{}
In the second formulation, each of the \eqncsref{ufl-ca2} is disaggregated into n new constraints, leading to constraints \eqncsref{ufl-cb2}.

\begin{eqnarray}
\mbox{minimize}&\sumdomain[i]{M}{\brak{f_i\cdot y_i+\sumdomain[j]{N}{c_{i,j}\cdot x_{i,j}}}}\eqnlab{ufl-mb}\\
\mbox{subject to}&\forall j\in N:\sumdomain[i]{M}{x_{i,j}}=1\eqnlab{ufl-cb1}\\
&\forall i\in M,j\in N:x_{i,j}\leq y_i\eqnlab{ufl-cb2}\\
&\forall i\in M,j\in N:x_{i,j},y_i\in\accl{0,1}\eqnlab{ufl-cb3}
\end{eqnarray}

Which of these formulations is preferable is not a matter of comparing the number of constraints or variables. In fact, as will be shown in the sequel, large formulations with many constraints and/or variables are usually better from a solver's point of view. This depends on the techniques that are used to solve the problem. Since these techniques very often rely heavily on linear programming, the quality of the formulation depends almost always on the accuracy of the linear programming relaxation, i.e., the problem which results when the integrality constraints are removed. For the uncapacitated facility location problem formulation (UFL2) is better than (UFL1), since the constraints \eqncsref{ufl-cb2} imply the constraints \eqncsref{ufl-ca2}.

\section{Combinatorial Optimization: a general formulation}
\seclab{generalform}
In this section we (informally) argue that each combinatorial optimization problem can be formulated as an integer program. In a combinatorial optimization problem a finite ground set $E$ is given. To each
element $e\in E$ a weight we is attached. A family $\calS$ of subsets of $E$ is identified as the set of feasible solutions. This family depends on the particular problem. The weight of a set $E'\subseteq E$ is the cumulative weight of its elements, i.e., $\fun{w}{E'}=\isumdomain[e]{E'}{w_e}$. The associated optimization problem is to find the maximum (or minimum) weight feasible solution E' ∈ S, i.e.,

\begin{equation}
\displaystyle\max_{E'\in\calS}\accl{\fun{w}{E'}}
\end{equation}

\paragraph{}
The set of feasible solutions $\calS$ is usually given implicitly. It is described by the properties of feasible solutions; it may be very large. For instance, in case of the matching problem, the ground set equals the set of edges, and the set $\calS$ is the collection of edge-sets that are matchings.In case of the knapsack problem, the ground set equals the set of items, and the set $\calS$ equals the collection of item-sets that can be put together in the knapsack.

\paragraph{}
Many examples of problems that fit in the above formulation are found in graph theory (\secref{formulations}). Among them are well-known problems like the shortest path problem, the minimum spanning tree problem, and the traveling salesman problem. The shortest path problem is defined as follows. In a graph $G=\tupl{V,E}$ the feasible solutions are the subsets of the edges that form paths between two specified vertices $s$ and $t$. Among the paths between $s$ and $t$ we want to find the one with a minimum number of edges, or if a length function is given on the edges, we want to find a path of minimum total length. In the minimum spanning tree problem a graph $G=\tupl{V,E}$ is given together with a weight function on the edges. A feasible solution is a set of edges that forms a tree. Among the trees we want to find one with minimum weight. This problem is easily solvable by a greedy algorithm, as is well known. However, if we restrict the set of feasible solutions to trees that form paths, the problem becomes the Hamiltonian path problem, which is highly intractable. Thus, in general, problems do not become simpler when the set of feasible solutions is reduced.

\paragraph{}
To formulate a combinatorial optimization problem in mathematical terms, we introduce decision variables for all elements of the ground set $E$. Each decision variable denotes a choice, namely whether the corresponding element is chosen or not. So, a variable can have two values, which are usually taken from $\accl{0,1}$. A set $E'\subseteq E$ can be described by a binary vector $\vec{x}_{E'}=\brak{x_e}_{e\in E}$ with $n=\abs{E}$ components as follows:

\begin{equation}
\semboolvar{x_e}{if element $e$ is in $E'$;}{otherwise.}
\end{equation}

\paragraph{}
The finite set of vectors $X\subseteq\RRR^n$ corresponding to feasible solutions from $\calS$ can then be described by means of constraints. In many cases, these constraints are linear and involve binary variables. The objective function $\vec{w}$ is usually a linear function of the components of $\vec{x}\in X$. The problem is then

\begin{equation}
\max\condset{\vec{w}\cdot\vec{x}}{A\cdot\vec{x}\leq\vec{b}\wedge \vec{x}\in\accl{0,1}^n}
\end{equation}
where $A$ and $\vec{b}$ depend on the problem at hand. This type of formulation is often called an Integer (or Binary) Linear Program, (ILP).

\section{General Combinatorial Optimization Problems}
\seclab{generalcombinatorial}

In the first section, we introduced binary variables to model decisions of the yes-no type, more specifically, to decide whether an element is in a set or not. These variables were used further to describe the constraints that determine the feasible solutions. In all the examples these constraints could be written as linear functions with a bound imposed on them. Similarly, the objective could be formulated as a linear function of the variables. In this section we will generalize the formulations of combinatorial problems, with respect to all three items, i.e., the variables, the constraints, and the objective.

\paragraph{}
The decisions in combinatorial problems may be more complicated than simple yes/no decisions. One may have to introduce integer variables or even real variables, like in linear programming, to model certain decisions. And, of course, combinations of these types of variables are possible in a single problem formulation. The combinatorial nature (finite or countable number of feasible solutions) may not be so evident in these problems. However, the number of ``interesting'' feasible solutions is usually still finite in such problems. For instance, in linear programming, the interesting feasible solutions are the vertices
of a polyhedron. The number of vertices is usually finite.

\paragraph{}
One may consider any function of the variables with a bound imposed on it as a constraint. Moreover, logical compositions of constraints, like implications and disjunctions (logical ``or'') can be used to model
the restrictions of a problem. In abstracto, any relation on the variables that restricts the set of feasible solutions can be used as a constraint.

\paragraph{}
The objective of a problem can be any function of the variables, thus it is not restricted to linear functions.

\paragraph{}
The above observations lead to the following abstract formulation of optimization problems. We distinguish between the real variables, denoted by the vector $\vec{x}$, and the integral variables, denoted by the
vector $\vec{y}$. A formulation of a combinatorial optimization problem contains the following items:
\begin{itemize}
 \item a vector of $n$ real decision variables:
 \begin{equation}
  \vec{x}=\tupl{x_1,x_2,\ldots,x_n}
 \end{equation}
 \item a vector of $m$ integral decision variables:
 \begin{equation}
  \vec{y}=\tupl{y_1,y_2,\ldots,y_m}
 \end{equation}
 \item a set $C$ of $k$ constraints:
 \begin{equation}
  C=\accl{C_1,\ldots,C_k}
 \end{equation}
 \item an objective function on the variables:
 \begin{equation}
  \fun{f}{\vec{x},\vec{y}}
 \end{equation}
\end{itemize}

\paragraph{}
The set of feasible solutions consists of vectors $\tupl{\vec{x},\vec{y}}$ that satisfy all the constraints. Each variable has a domain $D$, usually the set of real numbers $\RRR$, or the set of integer $\ZZZ$. The solution space of the problem is the Cartesian product of the domains of the variables, i.e., $\RRR^n\times\ZZZ^m$.

\paragraph{}
For many solution techniques, especially the ones that we are going to discuss, it is of eminent importance to restrict the domains of the variables as much as possible. Some constraints imply lower and upper bounds on the value of a variable, directly or indirectly. For instance, binary variables have an explicit lower and upper bound. If this is the case, we usually take this into account in the problem formulation explicitly, i.e., if a real variable $x_i$ has a lower bound $l_i$ and an upper bound $u_i$ , then we describe its domain as $\fbrk{l_i,u_i}$; if $y_j$ is an integral variable, with a lower bound $l_j$ and an upper bound $u_j$ , then we describe its domain as $\accl{l_j,\ldots,u_j}$.

\section*{Exercises}
\begin{exercise}
Consider the stable set problem on an undirected graph $G=\tupl{V,E}$. Show that the formulation \eqnnrefr{stableset-m}{stableset-c2} is a correct formulation of the stable set problem.
\end{exercise}
\begin{exercise}
Consider the matching depicted in \figref{matching-red}. Is it maximal? Is it maximum?
\end{exercise}
\begin{exercise}
Consider the stable set depicted in \figref{stableset-red}. Is it maximal? Is it maximum?
\end{exercise}
\begin{exercise}
Consider an undirected graph $G=\tupl{V,E}$. A edge cover $E'$ is a subset of the edges such that each node is incident to at least one edge in $E'$. Formulate the problem of finding a minimum cardinality edge cover as an integer linear program.
\end{exercise}
\begin{exercise}
Consider an undirected graph $G=\tupl{V,E}$. A node cover $V'$ is a subset of the nodes such that each edge is incident to at least one node in $V'$. Formulate the problem of finding a minimum cardinality node cover as an integer linear program.
\end{exercise}
\begin{exercise}
A clique partitioning of an undirected, complete graph $G$ is a partitioning of the vertices into subsets $V_1,V_2,\ldots,V_k$ such that the subgraph induced by each $V_i$ is a complete graph itself ($\rangei[i]{1}{k}$). Consider now a complete graph $G$ and an arbitrary weight $w_{i,j}$ for each edge of the graph (notice that $w_{i,j}$ can be negative; in fact, the problem is only interesting when there are both positive and negative edge weights). The objective is to find a clique partitioning of maximal weight, that is, a clique partitioning such that the cumulative weight of the edges that have both vertices in one and the same component is maximum. Formulate this problem as an integer linear programming problem. (Hint: use a variable for each edge).
\end{exercise}
\begin{exercise}
Consider the TSP.
\begin{itemize}
 \item When the distance matrix C is known to be symmetric, can you simplify formulation \eqnnrefr{tsp-m}{tsp-c4} by ``merging'' \eqncsref{tsp-c1} and \eqnnref{tsp-c2}?
 \item Which of the two formulations given in \sscref{tsp} is stronger?
\end{itemize}
\end{exercise}
\begin{exercise}
Given is a set of axis-aligned rectangles in the plane. Each of these rectangles needs to be stabbed, either by a horizontal line or by a vertical line. The problem is to find a set horizontal and vertical lines, stabbing each rectangle at least once, with minimum cardinality. We call this the rectangle stabbing problem. Formulate this problem as an integer linear programming problem. (Hint: first, give a formulation for the instance depicted in \figref{rectanglestabbing}).
\end{exercise}
\begin{exercise}
Given an instance of the rectangle stabbing problem as depicted in \figref{rectanglestabbing}, what fractional solution is the optimal linear programming relaxation (of the integer program that you just wrote down) for this instance?
\end{exercise}

\importtikzfigure{rectanglestabbing}{A rectangle stabbing instance.}

\chapter{From linear to integer optimization}
\chplab{lintointopt}

\section{Two equivalent definitions of a polyhedron}
\seclab{defpolyhed}

Consider the feasible region depicted in \figref{polyhedron}. How to describe this object mathematically?

\importtikzfigure{polyhedron}{A feasible region.}

Here is one way: we can view the feasible region as the intersection of as a number of halfspaces, each halfspace defined by a linear inequality. For instance, in the case of the figure above, we can write
\begin{equation}
\accl{\tupl{x_1,x_2}\in\RRR^2: -x_1+5\cdot x_2\leq 20 \wedge 2\cdot x_1+x_2\leq 6\wedge 6\cdot x_1-x_2\leq 10\wedge x_1\geq 0\wedge x_2\geq 0}.
\end{equation}
So we need five inequalities to precisely describe the feasible region.

\paragraph{}
An alternative is to focus on the extreme vertices of the feasible region in \figref{polyhedron}. Indeed, we can alternatively write
\begin{equation}
\accl{\tupl{x_1,x_2}\in\funma{conv}{\tupl{0,0},\tupl{0,4},\tupl{\frac{10}{11},\frac{46}{11}},\tupl{2,2},\tupl{\frac{5}{3},0}}}.
\end{equation}

\paragraph{}
The first approach can be seen as the linear programming approach; here, we simply list all the inequalities that jointly define the feasible region. The second approach can be seen as the integer programming approach; then, we list the extreme vertices of the region, and define the feasible region as anything that is in the convex hull of these given vertices. The two approaches are equivalent: anything that can be written using inequalities, can be written as the convex hull of a number of points, and vice versa. Let us now take a more general point of view. Thus, a polyhedron $P$ can be defined in two equivalent ways. First, as the set of points in $\RRR^n$ that satisfy a finite set of linear constraints, i.e.,
\begin{equation}
P=\condset{\vec{x}\in\RRR^n}{A\cdot\vec{x}\leq b}.
\end{equation}

\paragraph{}
Second, $P$ can be defined as the set of points in $\RRR^n$ that are convex combinations of points of a finite set $X=\accl{\vec{x}_1,\vec{x}_2,\ldots,\vec{x}_K}$ plus nonnegative combinations of points of a finite set $Y=\accl{\vec{y}_1,\vec{y}_2,\ldots,\vec{y}_L}$, i.e.,
\begin{equation}
P=\funm{conv}{X}+\funm{cone}{Y}
\end{equation}
where
\begin{equation}
\funm{conv}{X}=\condset{\sumieqb[k]{1}{K}{\alpha_k\cdot\vec{x}_k}}{\forall k:\alpha_k\in\RRR^+\wedge\sumieqb[k]{1}{K}{\alpha_k}=1}
\end{equation}
and
\begin{equation}
\funm{cone}{X}=\condset{\sumieqb[l]{1}{L}{\beta_l\cdot\vec{x}_l}}{\forall l:\alpha_l\in\RRR^+}
\end{equation}

\paragraph{}
The representation theorem of Farkas, Minkowski, and Weyl (see e.g. Nemhauser and Wolsey\cite{citeulike:2212037}) proves that both definitions of a polyhedron are equivalent. Moreover, if we have a description of $P$ in one form, then we know that there is a description in the other form.

\paragraph{}
In the sequel, we will consider only polyhedra which are subsets of the positive orthant in $\RRR^n$ , i.e., $P=\condset{\vec{x}\in\RRR^n}{A\cdot\vec{x}\leq\vec{b}\wedge\vec{x}\geq\vec{0}}$. This guarantees, if $P$ is not empty, the existence of extreme points. If $X$ and $Y$ are minimal, then $X$ contains the extreme points, and $Y$ contains the extreme rays of $P$. A polytope is a bounded polyhedron, i.e., $Y=\emptyset$ and $\funm{cone}{Y}=\accl{\vec{0}}$. With a few exceptions, we will only consider polytopes in the sequel. In fact, most of the polytopes that we consider lie in the $n$-dimensional unit cube $\BBB^n=\condset{\vec{x}\in\RRR^n}{\vec{0}\leq\vec{x}\leq\vec{1}}$.

\paragraph{}
The way the representation theorem is used in combinatorial optimization is different from the way it is used in linear programming. In linear programming we are given a set of feasible solutions by means of a system of linear constraints $\condset{\vec{x}\in\RRR^n}{A\cdot\vec{x}\leq\vec{b}\wedge\vec{x}\geq\vec{0}}$. The representation theorem is used to show that the optimization problem $\max\condset{\vec{c}\cdot\vec{x}}{A\cdot\vec{x}\leq\vec{b}\wedge\vec{x}\geq\vec{0}}$ has an extreme point that is optimum. In combinatorial optimization we are given, implicitly, a description of a finite set of feasible solutions $\calS$. The set $\calS$ is usually described as the set of subsets of a certain ground set $E$, where these subsets satisfy certain properties. The elements from $\calS$ are described by vectors $X=\accl{\vec{x}_1,\vec{x}_2,\ldots,\vec{x}_K}$. The representation theorem is used to conclude that $\funm{conv}{X}$ can be described by linear constraints, i.e., $\funm{conv}{X}=\condset{\vec{x}\in\RRR^n}{A\cdot\vec{x}\leq\vec{b}\wedge\vec{x}\geq\vec{0}}$ for some matrix $A$ and some vector $\vec{b}$. The main goal of this chapter is to find a way to obtain this set of linear constraints.

\paragraph{}
The set of feasible solutions $X$ of a combinatorial optimization problem is usually described through a formulation with linear restrictions and integrality constraints on the variables, i.e., $X=\condset{x\in\RRR^n}{A\cdot\vec{x}\leq\vec{b}\wedge\vec{x}\geq\vec{0}\wedge\vec{x}\in\ZZZ^n}$. Notice that the formulation of $X$ as an Integer Linear Program is not unique. The linear programming relaxation of this formulation, denoted by $\funm{LPR}{A,\vec{b}}$, is $\condset{\vec{x}\in\RRR^n}{A\cdot\vec{x}\leq\vec{b}\wedge\vec{x}\geq\vec{0}}$. Clearly, $\funm{conv}{X}$ is a subset of $\funm{LPR}{A,\vec{b}}$. A first question is: how well does the linear programming relaxation of a formulation describe the convex hull of $X$? This gives a criterion to decide on which formulation is best for a certain problem. In general we choose the formulation that defines the smallest polyhedron containing $\funm{conv}{X}$. A second question is: how can we find constraints that improve the linear programming relaxation? Then, we are looking for constraints that are satisfied by all feasible solutions in $X$, but that cut off part of the polyhedron defined by the relaxation.

\paragraph{}
We will give (partial) answers to both questions. In the following section we identify formulations for problems that have the property that the linear programming relaxation is tight, i.e., it describes $\funm{conv}{X}$ exactly. Then we describe a method, first illustrated using the stable set problem, that improves the linear programming relaxation by adding linear constraints (called valid inequalities) that make the formulation tighter. Finally, we discuss a systematic way of obtaining these inequalities.

\section{Linear description of combinatorial problems}
\seclab{lindesccomb}
Consider the integer linear program
\begin{equation}
\max\condset{\vec{c}\cdot\vec{x}}{A\cdot\vec{x}=\vec{b}\wedge\vec{x}\geq\vec{0}\wedge\vec{x}\in\accl{0,1}^m},
\end{equation}
where $A$ is an $n\times m$ matrix of integers and $\vec{b}$ is an $n$-vector of integers. Suppose we solve the corresponding linear programming formulation. Of course, we would be quite fortuitous if the resulting values for the $\vec{x}$-variables would be integral. However, in some special cases to be discussed next, one can guarantee that the resulting solution is indeed integral.

\paragraph{}
When solving the linear programming formulation, we know from the simplex method that there is a regular $n\times n$ submatrix $B$ of $A$ such that $\overline{x}=B^{-1}\cdot\vec{b}$ is the optimum solution of the linear program. Denoting the columns of $B$ by $B_{\star,j}$ $j\in\accl{1,\ldots,n}$, Cramer's rule gives an explicit description of the $n$ basic variables $x_j$ $j\in\accl{1,\ldots,n}$ corresponding to the columns of $B$:

\begin{equation}
x_j=\displaystyle\frac{\funm{det}{B_{\star,1}|B_{\star,2}|\ldots|B_{\star,j-1}|\vec{b}|B_{\star,j+1}|\ldots|B_{\star,n}}}{\funm{det}{B}}.
\end{equation}

The upper determinant is integral. If the determinant of $B$ would be $\pm 1$, then $x_j$ is certainly integral. We call a matrix $B$ with that property unimodular (UM). Since the basis $B$ may vary with the objective $\vec{c}$ and the right-hand side $\vec{b}$, we would like to have a characterization of matrices $A$ for which each $n\times n$ submatrix $B$ is UM. Indeed, for such matrices, the polyhedron
\begin{equation}
P=\condset{\vec{x}\in\RRR^n}{A\cdot\vec{x}=\vec{b}\wedge\vec{x}\geq\vec{0}}
\end{equation}
has integral vertices.

\paragraph{}
Next, consider the integer linear program
\begin{equation}
\max\condset{\vec{c}\cdot\vec{x}}{A\cdot\vec{x}\leq\vec{b}\wedge\vec{x}\geq\vec{0}\wedge\vec{x}\in\accl{0,1}}.
\end{equation}
To put the associated linear programming problem in the form above we introduce a slack-vector $\vec{y}$ and we reformulate it as
\begin{equation}
\max\condset{\vec{c}\cdot\vec{x}}{A\cdot\vec{x}+\vec{y}\leq\vec{b}\wedge\vec{x},\vec{y}\geq\vec{0}}.
\end{equation}
To obtain a similar result on the integrality of $\vec{x}$ (and $\vec{y}$), we can demand that each $n\times n$ submatrix of the matrix ($A|I$) be unimodular. This, however, is equivalent to stating that all regular square submatrices (not only those with $n$ rows and columns) of the matrix $A$ have a determinant $\pm 1$. Matrices with this property are called totally unimodular (TUM), see the next definition.

\begin{definition}[Totally unimodular matrix]
A matrix $A$ is called \emph{totally unimodular} if each square submatrix of $A$ has determinant equal to $-1$, $0$, or $+1$.
\end{definition}

\paragraph{}
We have the following important theorem for TUM matrices.

\begin{theorem}
If a matrix $A$ is TUM, then the polyhedron $P=\condset{\vec{x}\in\RRR^n}{A\cdot\vec{x}\leq\vec{b}\wedge\vec{x}\geq\vec{0}}$ has integral vertices.
\end{theorem}

\paragraph{}
Clearly, a TUM matrix consists only of entries $0,\pm 1$. In the following theorem we characterize two types of matrices that are totally unimodular.
\begin{theorem}
Let $A$ be a matrix with entries in $\accl{-1, 0, 1}$ such that there are at most two nonzeros in each column. If there exists a partition of the rows of $A$ in two sets $R_1$ and $R_2$ such that
\begin{enumerate}
 \item each column with two nonzero entries of the same sign, has one of its entries in $R_1$ and one in $R_2$,
 \item each column with two nonzero entries of different sign, has either both of its entries in $R_1$ or both of its entries in $R_2$,
\end{enumerate}
then the matrix $A$ is TUM.
\begin{proof}
We use induction on the size of the submatrices. Each submatrix of size $1\times 1$ is trivially TUM. Let $C$ be a submatrix of size $k\times k$. We consider three cases.
\begin{enumerate}
 \item If $C$ has a column with no nonzeros it is singular;
 \item if $C$ has a column with at most one nonzero, its determinant can be expanded along that column, and total unimodularity follows from the induction hypothesis;
 \item finally, if $C$ has only columns with two nonzeroes, we split its rows in the subsets with the property described above. Adding up the rows in each subset results in two identical row vectors, which implies that the matrix is singular.
\end{enumerate}
\end{proof}
\end{theorem}

\paragraph{}
There are two important classes of matrices that satisfy the conditions of the above theorem, and therefore are TUM. The node-arc incidence matrix $A$ of a directed graph (digraph) has rows corresponding with
the nodes and columns corresponding with the arcs. The entry $A_{v,a}$ can obtain three possible values:
\begin{enumerate}
 \item $A_{v,a}=-1$ if vertex $v$ is the tail of arc $a$.
 \item $A_{v,a}=1$ if vertex $v$ is the head of arc $a$.
 \item $A_{v,a}=0$ if vertex $v$ and arc $a$ are not incident.
\end{enumerate}

\paragraph{}
The node-edge incidence matrix $A$ of a graph has rows corresponding with the nodes and columns corresponding with the edges. The entry $A_{v,e}$ can obtain two possible values:
\begin{enumerate}
 \item $A_{v,e}=1$ if vertex $v$ and edge $e$ are incident.
 \item $A_{v,e}=0$ if vertex $v$ and edge $e$ are not incident.
\end{enumerate}

\begin{corollary}
The node-arc incidence matrix of a digraph is TUM. The node-edge incidence matrix of a bipartite graph is TUM.
\end{corollary}

\paragraph{}
This implies that many optimization problems on (di)graphs can be solved with linear programming. Among them are the shortest path problem, the max-flow problem, the min-cost flow problem, and the matching problem on bipartite graphs. Finally, we will show by an example that general node-edge incidence matrices need not be TUM. Consider the complete graph on three vertices $K_3$. Its node edge
incidence matrix is:
\begin{equation}
\brak{\begin{array}{ccc}
1&1&0\\
1&0&1\\
0&1&1
\end{array}}
\end{equation}
The determinant of this matrix is $-2$. Now let us compare the ILP formulation of the maximum cardinality matching problem with its linear programming relaxation on this graph. A maximum matching consists of one edge, and thus has value $1$. The LP-relaxation has an optimum value of $1.5$, since each of the three variables can obtain the value $0.5$, without violating any of the linear constraints.

\section{Valid inequalities}
\seclab{validineq}
Consider the stable set problem. Given is an arbitrary graph $G=\tupl{V,E}$. The feasible solutions are the stable sets in $G$. These solutions are represented by binary $n$-vectors, where the components correspond to the vertices. The integer linear programming formulation is the following, see also \eqnnrefr{stableset-m}{stableset-c2}.

\begin{eqnarray}
\mbox{maximize}&\sumdomain[v]{V}{x_v}\eqnlab{stablesetr-m}\\
\mbox{subject to}&\forall\tupl{v_1,v_2}\in E:x_{v_1}+x_{v_2}\leq 1\eqnlab{stablesetr-c1}\\
&\forall v\in V:x_v\in\accl{0,1}\eqnlab{stablesetr-c2}
\end{eqnarray}

The stable set polytope $\funm{conv}{\mbox{SS}}$ is the convex hull of the feasible solutions of the above formulation, i.e., the set of vectors corresponding to stable sets. We will derive two classes of so-called valid inequalities, the clique inequalities and the odd-cycle inequalities. These valid inequalities will allow us to improve the solution found by the linear programming relaxation.

\begin{example}
Consider the following graph depicted in \figref{clique-graph} on five vertices. The nodes 3, 4 and 5 form a clique, i.e., a subset of the vertices that induces a complete graph. At most one of these nodes can be present in a stable set. In other words, each feasible solution must satisfy the inequality $x_3+x_4+x_5\leq 1$. Thus, when solving the linear programming relaxation, one could add this constraint to the formulation, thereby making the constraints $x_3+x_4\leq 1$, $x_3+x_5\leq 1$ and $x_4+x_5\leq 1$ obsolete.

\importtikzfigure{clique-graph}{Graph with cliques.}

\paragraph{}
Notice that the inequality $x_3+x_4+x_5\leq 1$ can be derived from the constraints in the formulation by rounding in the following way.

\begin{equation}
\begin{array}{rcrcrcc}
x_3&+&x_4&&&\leq&1\\
&&x_4&+&x_5&\leq&1\\
x_3&+&&&x_5&\leq&1\\\hline
2\cdot x_3&+&2\cdot x_4&&2\cdot x_5&\leq&3
\end{array}
\end{equation}
This gives $x_3+x_4+x_5\leq 1$. This way we can get $3$-cliques from $2$-cliques. The clique of the nodes $1$, $2$, $3$ and $4$ can not be derived by rounding constraints of the formulation. However, we can derive them from the constraints generated by the four $3$-cliques contained in $\accl{1,2,3,4}$.
\end{example}
\begin{example}
Consider the following graph depicted in \figref{odd-cycle}: It is a cycle consisting of five vertices. The following inequality is valid. At most two nodes of the cycle can be in a packing.
\begin{equation}
x_1+x_2+x_3+x_4+x_5\leq 2
\end{equation}
In general, for a cycle $C$ with an odd number of vertices in a graph $G$, the inequality $\isumdomain[v]{C}{x_v\leq\floor{\frac{1}{2}\abs{C}}}$ valid. Unfortunately, adding such a constraint does not guarantee that the resulting solution is necessarily integral, as the graph depicted in \figref{odd-cycle-claw} shows.

\importtikzfigure{odd-cycle}{Odd cycle.}

\importtikzfigure{odd-cycle-claw}{Odd cycle with 5-claw.}

In this graph the solution x = ( 25 , 52 , 25 , 25 , 52 , 15 ) satisfies all the constraints that we have found for the
stable set problem. However, the cumulative sum of the variables is larger than two, and the number of
nodes in a packing in this graph is at most two. Therefore, we still do not have a description of conv(SS)
for this graph. We will now strengthen the odd cycle inequality generated by the vertices 1, 2, 3, 4 and
5. To do so, we look at the following inequality and we try to find a value for α as high as possible, such that the inequality maintains its validity.

x1 + x2 + x3 + x4 + x5 + αx6 ≤ 2
If x6 = 0, then the inequality reduces to the odd cycle inequality and is valid for any value of α. If x6 = 1,
then node 6 is in the packing. But this implies that nodes connected to 6 can not be in the packing.
Thus, x1 = x2 = x3 = x4 = x5 = 0, and the inequality is valid for all α ≤ 2. Concluding, the inequality
remains valid for α = 2.
\end{example}
Application: generalized node packing
In structured problems with many inequalities one can sometimes derive so-called implications, i.e., if one
variable has value 1 that implies that some other variable should have value 0. Such implications can lead
to strong inequalities based on node packing inequalities which tighten a formulation. This technique is
often used as a preprocessing step in solving huge integer linear programming problems.
Example
Three binary variables x1 , x2 and x3 satisfy the following relations.
−3x2

−

2x3

≤

−2

−4x1

−

3x2

−

3x3

≤

−6

2x1

−

2x2

+

6x3

≤

5

By using the complementing variables y1 = 1 − x1 , y2 = 1 − x2 and y3 = 1 − x3 , this system can be
reformulated into a system consisting of knapsack constraints, as follows.

3y2

+

2y3

≤

3

4y1

+

3y2

+

3y3

≤

4

2x1

+

2y2

+

6x3

≤

7

The node packing graph defined by the variables and constraints consists of the following vertices and
edges. There are two nodes for each variable, one representing the variable xi and the other its complement
yi . Two nodes are connected by an edge if the corresponding pair of variables sums up to a value of at

most 1. In this example the edges are {xi , yi } (i = 1, 2, 3); {yi , yj } (1 ≤ i < j ≤ 3) (second constraint);
{x1 , x3 } (third constraint); {y2 , x3 } (third constraint).

Figure 2.5: Generalized Node Packing graph
There is a clique consisting of the vertices corresponding to x3 , y3 , and y2 . Thus, x3 + y3 + y2 ≤ 1. Since
x3 + y3 = 1 this gives y2 = 0 and x2 = 1. Notice that the vector (x1 , x2 , x3 ) = (1, 21 , 12 ) satisfies the
original linear programming relaxation, but is cut off by the generated clique constraint.

\section{Gomory-Chv\'atal rounding}
\seclab{gomorychvatal}

A general method for finding valid inequalities that really cut off a part from the linear programming
relaxation is Gomory-Chv´atal rounding.
Suppose we have the following description of a set of points
X = {x ∈ IRn | Ax ≤ b; x ≥ 0; x integral}

Then, for any vector u ≥ 0, the following constraint is valid:
⌊uA⌋x ≤ ⌊ub⌋

By the nonnegativity of x we have that ⌊uA⌋x ≤ ub, and from the integrality of x it follows that
the left hand side is integral, and therefore we are allowed to round down the right hand side. By
29

adding these constraints for all nonnegative vectors u, we get the polyhedron P 1 = {x ∈ IRn | x ≥
0; for all u ≥ 0 ⌊uA⌋x ≤ ⌊ub⌋} which is really smaller than P . The problem is that there are infinitely
many constraints from which only few are necessary. Moreover, the resulting polyhedron P 1 may still
contain fractional vertices. Then the process of rounding can be repeated. Chv´atal showed that this
process gives the convex hull of all integral vectors in P after a finite number of times. The number of
times that these cuts must be added is called the Chv´atal-rank.
Example Consider the following integer program:
max

x2
3x1

+

2x2

≤

6

−3x1

+

2x2

≤

0

x1 ,

x2

≥

0

x1 ,

x2

integral.

The feasible region of the LP-relaxation of this integer program is depicted in Figure 2.6.

✡❏
✡ ❏
✡ ✉ ❏
❏
✡
❏
✡
❏
✡
❏
✡
❏
✡
✉
✉
❏✉
✡
Figure 2.6: Feasible region of LP-relaxation
5
Consider now the nonnegative vector u = ( 12

1
12 ).

Using u, we can construct the following linear

combination of the given constraints:
5
12

×

[3x1

+

2x2

≤

6]

1
12

×

[−3x1

+

2x2

≤

0]

x1

+

x2

≤

⌊ 30
12 ⌋ = 2.

1
Another possibility is to set u = ( 12

5
12 ).

The resulting constraint can be found as follows:
30

1
12

×

[3x1

+

2x2

≤

6]

5
12

×

[−3x1

+

2x2

≤

0]

−x1

+

x2

≤

6
⌊ 12
⌋ = 0.

When both additional restrictions are added to the original constraints, a polytope with integral vertices
arises. Thus, when these constraints are added to the linear program, the problem can be solved with
the simplex method.

✡❏
✡ ❏
✡ ✉ ❏
✡  ❅ ❏
✡  
❅❏
✡ 
❅❏
✡ 
❅❏
❏
✡ 
❅
❏✉
✡
 
❅
✉
✉
Figure 2.7: New polyhedron with additional constraints

A main question is of course: how to get hold of a “right” u? That is, a u such that the resulting inequality cuts away fractional extreme points without eliminating integral extreme verrtices? A constructive
method has been designed by Gomory. We continue our example described above to illustrate the idea.
When we solve the LP-relaxation of the formulation given in the example with the simplex-method (see
e.g. Chv`atal (2)), we start with the following dictionair:

x3

= 6

x4

=

z

=

− 3x1

− 2x2

3x1

− 2x2
x2 .

After performing two iterations, we find the following, final, dictionair:
31

x1

=

1

−

1
6 x3

+

1
6 x4

x2

=

3
2

−

1
4 x3

−

1
4 x4

z

=

3
2

−

1
4 x3

−

1
4 x4 .

Consider now the second equation of this last dictionair. We can rewrite it as follows:
1
3
1
x2 + x3 + x4 = .
4
4
2

(2.4)

Since x3 , x4 ≥ 0, it follows from the latter equality that:
1
1
3
x2 + ⌊ ⌋x3 + ⌊ ⌋x4 ≤ .
4
4
2
Moreover, since we know that the left-hand side of this expression is integral, we can safely round down
the right-hand side, and obtain:
1
1
x2 + ⌊ ⌋x3 + ⌊ ⌋x4 ≤ 1.
4
4
This is the same as writing
x2 ≤ 1.

(2.5)

We now continue our argument by subtracting inequality (2.5) from equality (2.4). This gives us:
1
1
1
x3 + x4 ≥ .
4
4
2
This is called a Gomory's cut. Substituting for x3 and x4 their defining equations (see the first simplex
dictionair), we obtain, an equivalent inequality:
x2 ≤ 1.
Notice that the current fractional solution violates this inequality. Repeatedly adding such a Gomory's
cut gives in the end an integral solution.

We now show that the procedure outlined above not only works for this particular example, but in fact
works in general. We denote by B the set of basic variables, and use xB(i) to denote the basic variable
corresponding to row i of the dictionair. Consider row i from the last dictionair:
dij xj = di .

xB(i) +
j ∈B
/

32

(2.6)

Since xj ≥ 0 for all j, it follows that
⌊dij ⌋xj ≤ di .

xB(i) +
j ∈B
/

Next, due to integrality of the left-hand side, we can round down the right-hand side, and obtain:
⌊dij ⌋xj ≤ ⌊di ⌋.

xB(i) +

(2.7)

j ∈B
/

When we define fij = dij − ⌊dij ⌋, and fi = di − ⌊di ⌋, and when we subtract (2.7) from (2.6), we get
Gomory's cut:
fij xj ≥ fi .
j ∈B
/

Notice that when di is not integral, then fi > 0, and hence Gomory's cut is violated by the current
solution (which, of course, has xj = 0 for all j ∈
/ B).
Unfortunately, there are some disadvantages to Gomory's method.

(a) The number of additional constraints is usually large. Moreover, Gomory's method does not always
yield the best constraints possible.
(b) The additional constraints contain a lot of non-zero elements, in general. Therefore problems may
arise with respect to the numerical stability of the simplex method.

In order to cope with these disadvantages we will develop methods for generating better constraints. Such
methods are usually problem specific, but they alleviate, to some extent, the problems mentioned.

Exercises
Exercise 1
Prove Corollary 2.3, i.e., show that the node-arc incidence matrix of a digraph is TUM, and that the
node-edge incidence matrix of a bipartite graph is TUM. Hint: use Theorem 2.2
Exercise 2
A 0-1 matrix has the consecutive ones property if each row has its entries with value 1 in neighboring
33

columns. Such a matrix is also called an interval matrix. Show that such matrices are TUM. Hints: use
induction, plus the facts that (i) interchanging two rows does not change whether a matrix is TUM, and
(ii) replacing row a by the difference of rows a and a′ also does not change whether a matrix is TUM.
Exercise 3
Consider Exercise 8 of Chapter 1, and let us modify that problem by only considering vertical lines as
potential stabbing lines. The resulting problem can be formulated as follows: given a set of intervals,
find a minimum number of vertical lines stabbing each interval at least once.
• Give an IP-formulation of the resulting problem.
• Is the corresponding constraint matrix an interval matrix (see Exercise 2)?
• How would you solve this problem?
• Can you use the same approach for the rectangle stabbing problem?

Exercise 4
Consider the problem formulated below, where in addition, the variables x1 and x2 need to be integral.
max

2 x1

+

x2

4 x1

+

x2

≤

8

x1

+

3 x2

≤

6

≥

0

≥

0

x1
x2

Solving the linear program by the simplex method gives the following final dictionary.

x1

=

18
11

−

3
11 x3

+

1
11 x4

x2

=

16
11

+

1
11 x3

−

4
11 x4

z

=

52
11

−

5
11 x3

−

2
11 x4

(2.8)

Find two Gomory cuts for the problem where the variables x1 and x2 need to be integral. Next, rewrite
each of these cuts in terms of solely x1 and x2 .
34

Exercise 5
Consider the following problem in integral variables x1 and x2 . How many cutting planes do you have to
add to solve the problem for k = 2? For k = 4? For general k? Can you use integer rounding to generate
a valid inequality? Hint: draw a picture.
max

x2
x1

−

2k x2

≥

0

2k x1

+

x2

≤

2k

≥

0

x1
Exercise 6

Let G = (V, E) be an undirected graph. A matching in G is represented by a vector x ∈ B |E| in the
following way. For each edge e ∈ E we define

 1 if edge e in the matching,
xe =
 0 otherwise.

The set of matchings is now given by

M = {x ∈ R|E| |

xe ≤ 1 (v ∈ V ), xe ∈ {0, 1} (e ∈ E)}.
e∈δ(v)

a) Derive with integer rounding that
xe ≤
e:e={i,j}

with

i,j∈S

|S| − 1
2

(2.9)

is a valid inequality for each S ⊆ V with 3 ≤ |S| ≤ |V | and |S| odd. (These inequalities are called
odd-set constraints).
b) Find, for the graph depicted below, a fractional solution violating the inequality (2.9) with S =
{1, 2, 3}.
Exercise 7
Consider the integer programming formulation of the clique partitioning problem (see Exercise 6 of
Chapter 1). Figure 2.9 shows an instance of that problem. Notice that solid lines correspond to edges
with weight +1, whereas dotted lines correspond to edges with weight −1. What is the value of the linear
programming relaxation of the integer programming formulation corresponding to this instance? Can
you find a violated inequality using integer rounding?
35

✉
 ❅
 
❅
 
❅
 
❅
 
❅
✉
 
❅✉
Figure 2.8: A triangle

✁
✁
✁✉
❍
❍

✁
✁

✁
✁

❍
❍

✁
✁

✉
✁❆
✁
✁ ❆

❆
❆

❍
✟
❍✉
✟

❆
❆

❆
❆

✟
✟

❆
❆

❆❆✉
✟
✟

Figure 2.9: An instance of clique partitioning

36

\bibliographystyle{alpha}
\bibliography{biblio}

\end{document}