\chapter{Column generation and branch-and-price}

In this chapter we describe a technique that is used for solving linear programs with a huge number of variables, namely \concept{column generation} (see \secref{columngen}). The use of this technique within an \concept{enumerative framework} is next discussed in \secref{branchprice}. We illustrate these issues on the \concept{cutting stock problem} (\concept{bin packing}) and the \concept{crew scheduling problem}.

\section{Column generation}
\seclab{columngen}

In this section we show how column generation procedures can work. We emphasize that column generation is a technique for solving (large) linear programming problems. The main idea is \begin{inlineenumerate}\item to work in iterations, and deal, at all times during the course of the method, with only a limited number of variables, and \item to \concept{verify optimality} of the outcome of each iteration using \concept{complementary slackness}\end{inlineenumerate}. In \sscref{cuttingstock} we show how this method can be applied to the \concept{cutting stock problem}, and in \sscref{crewscheduling}, we show how it works for a crew scheduling problem.

\subsection{The cutting stock problem}
\ssclab{cuttingstock}

We start by illustrating this on the \concept{cutting stock problem}. This problem can be described as follows.
\begin{definition}[Cutting stock problem]
Given are $n$ item types; there is a demand of $d_i$ items for type $i$, and each item of type $i$ has size $w_i$, $i=1,2,\ldots,n$. Let the total number of items be denoted by $D=\sum_i d_i$. Given are a (large enough) number of rolls of size $L$. The problem is to fulfill demand exactly, using a minimum number of rolls.
\end{definition}

\paragraph{}
A straightforward formulation is as follows, where we use, with $i=1,\ldots,D$, $j=1,2,\ldots$. binary variables:
\begin{equation}
\semboolvar{x_{ij}}{if item $i$ is cut from roll $j$;}{otherwise.}
\end{equation}
and
\begin{equation}
\semboolvar{y_j}{if roll $j$ is used;}{otherwise.}
\end{equation}
\paragraph{}
The formulation is now:
\begin{eqnarray}
\mbox{maximize}&\sumieqb[j]{1}{\infty}{y_j}\eqnlab{cutting-m}\\
\mbox{subject to}&\forall j\in\accl{1,2,\ldots}:\sumieqb[i]{1}{D}{w_i\cdot x_{ij}}\leq L\cdot y_j\eqnlab{cutting-c1}\\
&\forall i\in\accl{1,2,\ldots,D}:\sumieqb[j]{1}{\infty}{x_{ij}}=1\eqnlab{cutting-c2}\\
&\forall i\in\accl{1,2,\ldots,D},j\in\accl{1,2,\ldots}:x_{ij}\in\accl{0,1}\eqnlab{cutting-c3}\\
&\forall j\in\accl{1,2,\ldots}:y_j\in\accl{0,1}\eqnlab{cutting-c4}
\end{eqnarray}

\paragraph{}
There are a number of reasons why this formulation is not suited for solving large instances of the \concept{cutting stock problem}. First of all, the linear programming bound is very weak, since the solution $x_{ij}=\dfrac{1}{D}$ is feasible to the linear programming relaxation (verify that this leads to a linear programming solution with value ($\sumieqb[i]{1}{\infty}{w_i/L}$)). Secondly, symmetry of solutions (the rolls are interchangeable) will also affect the performance of a \concept{branch-and-bound algorithm} based on this formulation. Therefore, another formulation based on another concept is welcome. Instead of focusing on individual items, let us focus on a possible way to cut a roll. More precisely, we refer to a possible way of cutting a roll as a \concept{pattern}. Obviously, given the input described above, we could, in principle enumerate all possible patterns. Then, we could write down the following formulation that uses an integral variable $z_j$ for the number of times a roll is cut according to \concept{pattern} $j=1,2,\ldots$:

\begin{eqnarray}
\mbox{maximize}&\sumieqb[j]{1}{\infty}{z_j}\eqnlab{cuttingb-m}\\
\mbox{subject to}&\forall i\in\accl{1,2,\ldots,n}:\sumieqb[j]{1}{\infty}{a_{ij}\cdot z_j}=d_i\eqnlab{cuttingb-c1}\\
&\forall j\in\accl{1,2,\ldots}:z_j\in\NNN\eqnlab{cuttingb-c2}
\end{eqnarray}

Notice that $a_ij$ denotes the number of times that an item of type $i$ is used in pattern $j$; this is a known number. Now although this formulation does not suffer from the disadvantages mentioned above, there is a disadvantage to the current formulation: the number of variables. Indeed, this number can be astronomically high.

\paragraph{}
Let us therefore first focus on solving the linear programming relaxation of formulation \eqnnrefr{cuttingb-m}{cuttingb-c2}. How to overcome the obstacle posed by the number of variables? The crucial idea of column generation is based on the observation that in an optimal solution of the linear programming relaxation of \eqnnrefr{cuttingb-m}{cuttingb-c2}, only very few of these variables will have a value different from $0$. Indeed, the theory of the simplex-algorithm tells us that if there is an optimal solution, this solution will consist of $n$ basic variables that may have a non-zero value, while all other variables will be non-basic and have value $0$. How to make use of this observation? Of course, we do not know a priori which variables are basic in an optimal solution and which variables are not. However, given a feasible basic solution, we can determine (using complementary slackness and duality) whether it is optimal, and if not, which variable should be included in the basis to improve the current solution. This idea suggests an iterative procedure for solving the LP-relaxation of \eqnnrefr{cuttingb-m}{cuttingb-c2} as follows:

\begin{stepenum}
 \item \stplab{colgen-a}Start with a subset of the variables (that contains a feasible solution). All other variables have implicitly the value $0$. This is called the \concept{restricted master problem}.
 \item \stplab{colgen-b}Solve the LP-relaxation, and arrive at a \concept{feasible primal solution}.
 \item \stplab{colgen-c}Question: does there exist a variable (a pattern) with \concept{negative reduced costs}? That is, does there exist a variable that should enter the basis? This question is called the \concept{pricing problem}. If no, the current LP-solution is optimal, and we STOP, else
 \item \stplab{colgen-d}Identify that variable and add it to the current set of variables. Go to \stpref{colgen-a}.
\end{stepenum}

\paragraph{}
At first sight, there may be no reason why this would be more efficient than solving the linear program with all variables. Indeed, the crux of this approach lies in solving the \concept{pricing problem}: if explicitly determining the \concept{reduced cost} of each individual variable by enumeration was the only way to solve the pricing problem, we would not have gained much. However, in a lot of cases, solving the \concept{pricing problem} can be done very efficiently, for instance by solving a \concept{shortest path problem}, or by solving a \concept{knapsack problem}. Then, \concept{column generation} is a very efficient way of solving the master problem. Let us proceed by illustrating this idea on the \concept{cutting stock problem}. Consider a column in the matrix $A$ of the second cutting stock formulation, and let us denote this column by $\vec{a}$. It describes a \concept{pattern}, that is, the entries of that column equal the \concept{multiplicity} of each item type in that \concept{pattern}. Of course, for a column $\vec{a}=\tupl{a_1,a_2,\ldots,a_n}$ of $A$ to be a \concept{feasible pattern} it must be true that $\sumieqb[i]{1}{\infty}w_i\cdot a_i\leq L$. But the converse is true as well, that is, any $n$ nonnegative integers $\tupl{a_1,a_2,\ldots,a_n}$ that satisfy $\sumieqb[i]{1}{\infty}w_i\cdot a_i\leq L$ is a \concept{feasible pattern}. Let us denote pattern $j$ by nonnegative integers $\tupl{a_{1j},a_{2j},\ldots,a_{nj}}$.

\paragraph{}
Now suppose we are given a \concept{primal solution} $\vec{z}=\tupl{z_1,z_2,\ldots}$, and we are also given the values of the \concept{dual variables} $\tupl{u_1,u_2,\ldots,u_n}$ associated to the constraints \eqnnref{cuttingb-c1}. How to determine whether $\vec{z}$ is an \concept{optimal solution} to the LP-relaxation of \eqnnrefr{cuttingb-m}{cuttingb-c2}? Let us consider the constraints of the dual of the LP-relaxation of \eqnnrefr{cuttingb-m}{cuttingb-c2}. We know that these constraints are of the form: $\sumieqb[i]{1}{\infty}u_i\cdot a_ij\leq1$ for each \concept{possible pattern} $j$ (verify this!). Thus, if the current $\vec{u}$-values satisfy all these constraints (i.e., for each \concept{pattern} $j$), we have an \concept{optimal LP-solution}. Or, alternatively formulated, we are looking for a \concept{pattern} $\tupl{a_1,a_2,\ldots,a_n}$ such that $\sumieqb[i]{1}{\infty}w_i\cdot a_i\leq L$ and $\sumieqb[i]{1}{\infty}u_i\cdot a_i>1$. If we can find such a \concept{pattern}, we know that the current LP-solution is not optimal, and hence we add this \concept{pattern} to the current set of columns and start a new iteration. Otherwise, if we cannot find such a \concept{pattern}, the current LP-solution is optimal. Summarizing, the \concept{pricing problem} boils down to solving the following problem:

\begin{eqnarray}
\mbox{maximize}&\sum_[i]{u_i\cdot a_i}\eqnlab{pricing-m}\\
\mbox{subject to}&\sum_[i]{w_i\cdot a_i}\leq L\eqnlab{pricing-c1}\\
&\forall i:a_i\in\NNN\eqnlab{pricing-c2}
\end{eqnarray}

This problem is (a variant of) the well-known \concept{knapsack problem} (which, in spite of its \concept[NP-hard]{NP-hardness}, can be solved in reasonable computing times for very large instances, see \chpref{lifting}). Concluding, the \concept{pricing problem} can be solved relatively fast, and hence the LP-relaxation of \eqnnrefr{cuttingb-m}{cuttingb-c2} can be solved fast as well.

\subsection{The hierarchical crew scheduling problem}
\ssclab{crewscheduling}

In this subsection we illustrate the \concept{column generation} technique on the \concept{hierarchical crew scheduling
problem}. This problem can be described as follows.

\begin{definition}[hierarchical crew scheduling problem]
Given are $m$ crews who have to perform $n$ tasks. For each \concept{crew} $t$ a \concept{cost-rate} $r_t$ is known, $t=1,2,\ldots,m$. Each \concept{task} is characterized by a \concept{starting time} $s_i$ and a \concept{processing time} $p_i$. Each task $i$ has to start at $s_i$ and must be carried out \concept{nonpreemptively} by some crew until $s_i+p_i$. Also, a \concept{distance} $d_{ij}$ for every pair of tasks $i$ and $j$ is given. Finally, the crews are \concept{hierarchically ordered} in the following way: for each job a 'maximal' crew is given such that each crew with an index higher than the index of the maximal crew is not capable of performing that job; all other crews are capable of performing that job. The cost of a crew is the product of its cost rate $r_t$ and the distance traveled by that crew. The problem is to find an assignment of tasks to crews against minimal costs.
\end{definition}

\begin{example}
Here is an instance with $2$ crews and $3$ jobs:
\begin{equation}
m=2, n=3, r_1=3, r_2=1, s_1=20, s_2=40, s_3=60, p_1=p_2=p_3=0
\end{equation}
job $1$ can only be carried out by crew $1$, jobs $2$ and $3$ can be carried out by each of the two crews and with distances $d_{ij}$ as in \figref{hcsp-distances}.
\importtikzfigure{hcsp-distances}{The distance of the hierarchical crew scheduling problem example.}
\end{example}

\paragraph{}
One (of the~$2$) optimal solution(s) for this instance can be described as follows. Starting at time~$0$, crew~$1$ travels to job~$1$, waits~$4$ time units, performs the job, travels to job~$3$, waits~$28$ time units, performs it and returns to the depot. Distance traveled is~$37$, so its costs are~$111$. Crew~$2$ simply travels to job~$2$, performs it and returns for a total cost of~$20$. Thus, the value of an optimal solution to this instance is~$131$.

\paragraph{}
We model the \concept{hierarchical crew scheduling problem} as a problem on a \concept{weighted, directed graph} $G=\tupl{V,A}$ as follows. Construct a \concept{vertex} for each task $i=1,\ldots,n$ and let $V=\accl{1,\ldots,n}\cup\accl{s,f}$. The vertices $s$ and $f$ can be regarded as the \concept{source} and \concept{sink} of the \concept{graph} $G$. There is an \concept{arc} from \concept{vertex} $i$ to $j$ in $A$ if $s_i+p_i+d_{ij}\leq s_j$ for all $i,j\in V\setminus\accl{s,f}$. Further, there is an \concept{arc} $\tupl{s,i}$ and $\tupl{i,f}$ in $A$ for all $i\in V\setminus\accl{s,f}$. Finally, there is a cost vector $c_{ij}^t$ associated to each \concept{arc} $\tupl{i,j}\in A$. We compute this \concept{vector} as follows:

\begin{equation}
\semvalvar{c_{ij}^t}{r_t\cdot d_{ij}}{if crew $t$ is able to do jobs $i$ and $j$,}{M}{otherwise, for all $t$, for all $\tupl{i,j}\in A$}
\eqnlab{semcijt}
\end{equation}

where $M$ is a large number.

\paragraph{}
Consider now the following formulation using the following parameters:
\begin{itemize}
 \item R is the set of paths in $G$ from $s$ to $f$,
for $i=1,2,\ldots,n$, $r\in R$:

\begin{equation}
\semboolvar{\delta_{ir}}{if vertex $i$ is in path $r$, and}{otherwise}
\end{equation}

 \item for $t=1,2,\ldots,n$, $r\in R$, $c_{tr}$ is the cost incurred when crew $t$ takes path $r$, (observe that, for a given $r$, $t$, we can easily compute this quantity using \eqnref{semcijt}; notice that if a path $r$ contains a vertex that cannot be served by crew $t$, we set the corresponding $c_{tr}$ to a large number), and using, for $t=1,\ldots,m$, $r\in R$, the decision variables
 
\begin{equation}
\semboolvar{y_{tr}}{if crew $t$ takes path $r$, and}{otherwise}
\end{equation}

\end{itemize}
We now introduce a new formulation based on the introduced variables:

\begin{eqnarray}
\mbox{minimize}&\sumieqb[t]{1}{m}{\sumdomain[r]{R}{c_{tr}\cdot y_{tr}}}\eqnlab{cghcsp-m}\\
\mbox{subject to}&\forall i\in\accl{1,2,\ldots,n}:\sumieqb[t]{1}{m}{\sumdomain[r]{R}{\delta_{ir}\cdot y_{tr}}}=1\eqnlab{cghcsp-c1}\\
&\forall t\in\accl{1,2,\ldots,m}:\sumdomain[r]{R}{y_{tr}}\leq1\eqnlab{cghcsp-c2}\\
&\forall t\in\accl{1,2,\ldots,m},r\in R:y_{tr}\in\accl{0,1}\eqnlab{cghcsp-c3}
\end{eqnarray}

Constraints \eqnnref{cghcsp-c1} state that each vertex must occur once in a selected path, inequalities \eqnnref{cghcsp-c2} express that a crew can do at most one path and constraints \eqnref{cghcsp-c3} are the \concept{integrality constraints}. The LP relaxation of this model is found by replacing constraints \eqnref{cghcsp-c3} by $y_{tr}\geq0$ for all $t,r$.

\paragraph{}
Given a \concept{feasible basis} for some LP the question determining whether this basis is an optimal one is: do there exist variables with negative \concept{reduced costs}? Using \concept{dual variables} $u_i$ attached to constraints \eqnnref{cghcsp-c1} and \concept{dual variables} $e_t$ to constraints \eqnnref{cghcsp-c2} we can deduce the following expression for the reduced costs of variable $y_{tr}$:

\begin{equation}
c_{tr}-\sumieqb[i]{1}{n}{\delta_{ir}\cdot u_i-e_t}
\end{equation}

Thus, given some LP-solution and its associated \concept{dual variables} the \concept{pricing problem} boils down to the following question:

\begin{equation}
\exists t,r\in R:c_{tr}-\sumieqb[i]{1}{n}{\delta_{ir}\cdot u_i-e_t}<0
\end{equation}

This question can be answered as follows:
\begin{lemma}
The \concept{price problem} can be solved by solving $m$ \concepts{shortest path problem} on a \concept{directed acyclic graph}.
\begin{proof}
We claim that for a fixed $t$, the \concept{price problem} boils down to a \concept{shortest path problem} which implies the lemma. This can be seen as follows: consider the \concept{graph} corresponding to the instance and consider only those nodes that can be visited by crew $t$. Observe that no \concepts{cycle} occur in this network. Modify the existing \concepts{arc cost} $c_{ij}^t$ by setting $h_{ij}:=c_{ij}^t-u_j$ for all $\tupl{i,j}\in A$. Observe that the cost of a path $P$ in this network with respect to costs $h_{ij}$ from $s$ to $f$ equals: $\sumdomain[\tupl{i,j}]{P}{h_{ij}}=\sumdomain[\tupl{i,j}]{P}{c_{ij}^t-u_j}=c_{tr}-\sum_i{\delta_{ir}\cdot u_i}$. If this expression is smaller than $e_t$, there is a \concept{profitable column} (\concept[profitable path]{path}) for crew $t$, otherwise not.
\end{proof}
\end{lemma}
This shows that the \concept{pricing problem}, and hence the associated \concept{LP-relaxation}, can be solved efficiently.

\section{Branch-and-price}
\seclab{branchprice}

Of course, it is nice that one can solve a \concept{linear program} with (exponentially) many variables efficiently using \concept{column generation}. However, this leaves us with a potentially \concept{fractional solution}. We are interested in \concepts{integral solution}. Can we embed \concept{column generation} within an \concept{enumerative framework} so that we are guaranteed to find an \concept{integral optimum}? In this section we describe such an approach.

\paragraph{}
Let us consider the example of the \concept{hierarchical crew scheduling problem}. A simple idea would be to argue as follows: given a \concept{fractional solution}, pick a variable with a fractional value. We know that in an \concept{optimal solution} this variable will have either value $0$ or value $1$. Create two subproblems, one subproblem in which that variable equals $1$, and another subproblem where this variable is constrained to be $0$. Unfortunately, this idea is too simple. Setting a variable to $1$ in the crew scheduling context poses no problem: indeed the problem becomes smaller since we postulate that, in this branch this crew $t$ takes path $r$. However, setting a variable to $0$ causes a difficulty: how can we guarantee that when we solve the \concept{pricing problem} this specific variable does \emph{not} come out as the variable to be added? One might think: ok let us find the \concept{shortest path}, and if it corresponds to this variable, find the second \concept{shortest path}. But that idea is bound to cause difficulties when we have a \concept{branching tree} that can have many \concepts{level}.

\paragraph{}
We need another partitioning of the solution space. Instead of branching by setting a variable to $1$ versus setting it to $0$, we need a \concept{branching rule} that does not destroy the \concept{efficient solvability} of the \concept{pricing problem}. For the \concept{hierarchical crew scheduling problem}, an example of such a rule is as follows: Here we propose a \concept{branching rule} that leaves the structure of the problem intact, allowing for the \concept{efficient solvability} of the \concept{pricing problem} throughout the search tree.

\paragraph{}
Suppose that $y$ is a fractional \concept{feasible LP-solution}, and let us call a \concept{path} $r$ from $s$ to $f$ a \concept{fractional path} (with respect to $y$) if there exists a $t$ with $0<y_{tr}<1$. We claim that $y$ has the following property: there exist two vertices $i$ and $j$ (that differ from \concept{source} and \concept{sink}) that lie consecutively on a \concept{fractional path} such that the sum of all $y_{tr}$ such that $r$ contains arc $\tupl{i,j}$ is greater than $0$ and smaller than $1$. Let us formally phrase this claim in the following lemma:

\begin{lemma}
If $y$ is fractional, there exist two nodes $i,j\in V\setminus\accl{s,f}$, $\tupl{i,j}\in A$ with $0<\sum_t{\sum_{r\in R,\tupl{i,j}\in r}y_{tr}}<1$.

\begin{proof}
Observe that if y is fractional there are at least two different fractional paths. Now, consider the
0<

t

r:{i,j}

first node in each fractional path. If this set of nodes has cardinality more than 1, the claim is easily seen
to be true. So assume that each fractional path has the same first node. However, then we can repeat
this argument replacing the sink s by this first node. Since there must be at least two fractional paths,
a pair i, j as described in the lemma must exist.
\end{proof}
\end{lemma}

Thus, by this Lemma, when y is fractional there exist nodes i and j (that differ from source and sink)
that are connected by an arc whose sum of fractional values lies between 0 and 1. Now, in an optimal
solution either these nodes are visited consecutively, or they are not. More specifically, given a fractional solution we identify two nodes i and j having the property described above. Then we branch as follows.
In one branch we modify G into G1 by deleting all arcs {i, p} for p = j and all arcs {p, j} for p = i. Thus,
in any feasible solution there is a path that contains arc {i, j}. In the other branch we modify G into G2
by simply deleting arc {i, j}. In this case it is obvious that no feasible solution has a path with arc {i, j}
in it. Notice that the current solution is excluded by this rule. Let us illustrate this branching rule on
the example.
Example (continued): Consider the instance described in the example. The graph corresponding to
this instance is depicted in Figure 5.2.

Figure 5.2: The graph G
An optimal solution to the LP-relaxation of (5.12)-(5.14) for this instance is described by y2,s−2−3−f =
y1,s−1−2−f = y1,s−1−3−f = 12 , and all other variables 0. Now, let 1,2 be a pair of nodes that we branch
on. The resulting graphs G1 and G2 are depicted in Figure 5.3:



Figure 5.3: The graphs G1 (left) and G2 (right)
Notice that in G1 the arcs {s, 2}, {1, 3} and {1, f } have been deleted. When solving the LP-relaxation
corresponding to G1 we find an integral solution with value 132. G2 is constructed by deleting arc {1, 2}.
In this branch we also find an integral solution with value 131, which is therefore optimal.
Concluding, we have found a branching rule for the hierarchical crew scheduling problem that preserves
69

the structure of the original problem. In this way, the efficient solvability of the pricing problem remains
intact throughout the nodes in the branching tree. In general, this is the challenge when devising
branching rules for integer programming problems whose LP-relaxation is solved by column generation.

Exercises
Exercise 1
Consider the following problem occurring in combinatorial auctions. Given are m items, and n bidders.
Each bidder j, 1 ≤ j ≤ n, specifies a (nonnegative) bid bj (S) for subsets of the items S ⊆ {1, 2, . . . , m}.
(Notice that the value of a bid that a bidder specifies for a pair of items need not be the same as the sum
of the valuations for the two individual items; this is, in fact, the defining property of a combinatorial
auction). Clearly, each item can be allocated to at most one bidder, and each bidder receives at most
one set of items.
(i) Give an integer programming formulation for this problem that maximizes the total auction revenue.
(ii) How many variables are there?
(iii) What is the dual of the linear programming relaxation of your formulation?
Exercise 2
Consider the following problem occurring in production management. Given are N jobs which have to be
processed by a single machine. Each job needs some (prespecified) tools to be processed; in total there
are M tools available. The machine can hold at most C tools simultaneously (and of course, each job
does not need more than C tools). We call a subset of the jobs a feasible group if these jobs together
require at most C tools. The job grouping problem consists in finding a minimum number of feasible
groups such that each job is contained in at least one group.
(i) Formulate this problem as an integer programming problem.
(ii) Describe a column generation approach for this problem.
Exercise 3
Consider the following problem occurring in routing applications. Given are n + 1 locations, among
70

which a depot, and distances d between each pair of locations. The depot harbors K vehicles that serve
the locations. No vehicle can serve more than C locations. The problem is to select K paths (one for
each vehicle), each starting at the depot, such that each location is in exactly one path. The goal is to
minimize the total length of the K paths.
(i) Formulate
 this problem as a ‘traditional' integer program using variables
 1 if vehicle k travels from location i to location j
xijk =
 0 otherwise

(ii) Formulate this problem as an integer programming problem using a formulation that involves
exponentially many variables.
(iii) Discuss how a column generation approach for this formulation would work.